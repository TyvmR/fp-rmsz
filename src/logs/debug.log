2023-03-07 15:58:54  [ main:0 ] - [ DEBUG ]  
Dig to clean the java.lang.String
2023-03-07 15:58:54  [ main:1 ] - [ DEBUG ]  
Dig to clean the [C
2023-03-07 15:58:54  [ main:15 ] - [ DEBUG ]  
Dig to clean the java.lang.Integer
2023-03-07 15:58:54  [ main:16 ] - [ DEBUG ]  
Dig to clean the java.lang.Integer
2023-03-07 15:58:54  [ main:16 ] - [ DEBUG ]  
Dig to clean the java.lang.String
2023-03-07 15:58:54  [ main:16 ] - [ DEBUG ]  
Dig to clean the [C
2023-03-07 15:58:54  [ main:16 ] - [ DEBUG ]  
Dig to clean the java.lang.Integer
2023-03-07 15:58:54  [ main:16 ] - [ DEBUG ]  
Dig to clean the java.lang.Long
2023-03-07 15:58:54  [ main:16 ] - [ DEBUG ]  
Dig to clean the java.lang.Long
2023-03-07 15:58:54  [ main:17 ] - [ DEBUG ]  
Dig to clean the java.lang.Boolean
2023-03-07 15:58:55  [ main:44 ] - [ DEBUG ]  
Dig to clean the org.apache.flink.api.common.eventtime.WatermarkStrategy$$Lambda$229/973604791
2023-03-07 15:58:55  [ main:44 ] - [ DEBUG ]  
Dig to clean the org.apache.flink.api.common.eventtime.TimestampAssignerSupplier$SupplierFromSerializableTimestampAssigner
2023-03-07 15:58:55  [ main:44 ] - [ DEBUG ]  
Dig to clean the org.fllik.geektime.flinksql.FlinkSQLOverWinEvRowRange$1
2023-03-07 15:58:55  [ main:150 ] - [ DEBUG ]  
Got FunctionDefinition 'max' from 'core' module.
2023-03-07 15:58:55  [ main:153 ] - [ DEBUG ]  
Got FunctionDefinition 'avg' from 'core' module.
2023-03-07 15:58:55  [ main:194 ] - [ DEBUG ]  
Cannot find FunctionDefinition 'date_time' from any loaded modules.
2023-03-07 15:58:55  [ main:197 ] - [ DEBUG ]  
Cannot find FunctionDefinition 'product_id' from any loaded modules.
2023-03-07 15:58:55  [ main:200 ] - [ DEBUG ]  
Cannot find FunctionDefinition 'product_id' from any loaded modules.
2023-03-07 15:58:55  [ main:201 ] - [ DEBUG ]  
Cannot find FunctionDefinition 'price' from any loaded modules.
2023-03-07 15:58:55  [ main:202 ] - [ DEBUG ]  
Got FunctionDefinition 'max' from 'core' module.
2023-03-07 15:58:55  [ main:206 ] - [ DEBUG ]  
Cannot find FunctionDefinition 'price' from any loaded modules.
2023-03-07 15:58:55  [ main:206 ] - [ DEBUG ]  
Got FunctionDefinition 'avg' from 'core' module.
2023-03-07 15:58:55  [ main:451 ] - [ DEBUG ]  
Plan after converting SqlNode to RelNode
LogicalProject(product_id=[$0], max_price=[MAX($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING)], avg_price=[/(CASE(>(COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), 0), $SUM0($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), null:DOUBLE), COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING))])
  LogicalTableScan(table=[[default_catalog, default_database, UnnamedTable$0]])

2023-03-07 15:58:55  [ main:1008 ] - [ DEBUG ]  
iteration: 1
2023-03-07 15:58:55  [ main:1021 ] - [ DEBUG ]  
Rule Attempts Info for HepPlanner
2023-03-07 15:58:55  [ main:1022 ] - [ DEBUG ]  

Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-07 15:58:56  [ main:1023 ] - [ DEBUG ]  
For final plan, using rel#7:LogicalSink.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#6,table=default_catalog.default_database.Unregistered_Collect_Sink_1,fields=product_id, max_price, avg_price)
2023-03-07 15:58:56  [ main:1023 ] - [ DEBUG ]  
For final plan, using rel#5:LogicalProject.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#4,inputs=0,exprs=[MAX($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), /(CASE(>(COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), 0), $SUM0($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), null:DOUBLE), COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING))])
2023-03-07 15:58:56  [ main:1023 ] - [ DEBUG ]  
For final plan, using rel#1:LogicalTableScan.NONE.any.None: 0.[NONE].[NONE](table=[default_catalog, default_database, UnnamedTable$0])
2023-03-07 15:58:56  [ main:1041 ] - [ DEBUG ]  
optimize convert table references before rewriting sub-queries to semi-join cost 17 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.Unregistered_Collect_Sink_1], fields=[product_id, max_price, avg_price])
+- LogicalProject(product_id=[$0], max_price=[MAX($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING)], avg_price=[/(CASE(>(COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), 0), $SUM0($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), null:DOUBLE), COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING))])
   +- LogicalTableScan(table=[[default_catalog, default_database, UnnamedTable$0]])

2023-03-07 15:58:56  [ main:1041 ] - [ DEBUG ]  
Rule Attempts Info for HepPlanner
2023-03-07 15:58:56  [ main:1041 ] - [ DEBUG ]  

Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-07 15:58:56  [ main:1041 ] - [ DEBUG ]  
For final plan, using rel#12:LogicalSink.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#11,table=default_catalog.default_database.Unregistered_Collect_Sink_1,fields=product_id, max_price, avg_price)
2023-03-07 15:58:56  [ main:1041 ] - [ DEBUG ]  
For final plan, using rel#10:LogicalProject.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#9,inputs=0,exprs=[MAX($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), /(CASE(>(COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), 0), $SUM0($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), null:DOUBLE), COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING))])
2023-03-07 15:58:56  [ main:1041 ] - [ DEBUG ]  
For final plan, using rel#1:LogicalTableScan.NONE.any.None: 0.[NONE].[NONE](table=[default_catalog, default_database, UnnamedTable$0])
2023-03-07 15:58:56  [ main:1042 ] - [ DEBUG ]  
optimize rewrite sub-queries to semi-join cost 0 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.Unregistered_Collect_Sink_1], fields=[product_id, max_price, avg_price])
+- LogicalProject(product_id=[$0], max_price=[MAX($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING)], avg_price=[/(CASE(>(COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), 0), $SUM0($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), null:DOUBLE), COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING))])
   +- LogicalTableScan(table=[[default_catalog, default_database, UnnamedTable$0]])

2023-03-07 15:58:56  [ main:1042 ] - [ DEBUG ]  
Rule Attempts Info for HepPlanner
2023-03-07 15:58:56  [ main:1043 ] - [ DEBUG ]  

Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-07 15:58:56  [ main:1043 ] - [ DEBUG ]  
For final plan, using rel#17:LogicalSink.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#16,table=default_catalog.default_database.Unregistered_Collect_Sink_1,fields=product_id, max_price, avg_price)
2023-03-07 15:58:56  [ main:1043 ] - [ DEBUG ]  
For final plan, using rel#15:LogicalProject.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#14,inputs=0,exprs=[MAX($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), /(CASE(>(COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), 0), $SUM0($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), null:DOUBLE), COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING))])
2023-03-07 15:58:56  [ main:1043 ] - [ DEBUG ]  
For final plan, using rel#1:LogicalTableScan.NONE.any.None: 0.[NONE].[NONE](table=[default_catalog, default_database, UnnamedTable$0])
2023-03-07 15:58:56  [ main:1043 ] - [ DEBUG ]  
optimize sub-queries remove cost 1 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.Unregistered_Collect_Sink_1], fields=[product_id, max_price, avg_price])
+- LogicalProject(product_id=[$0], max_price=[MAX($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING)], avg_price=[/(CASE(>(COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), 0), $SUM0($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), null:DOUBLE), COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING))])
   +- LogicalTableScan(table=[[default_catalog, default_database, UnnamedTable$0]])

2023-03-07 15:58:56  [ main:1044 ] - [ DEBUG ]  
Rule Attempts Info for HepPlanner
2023-03-07 15:58:56  [ main:1044 ] - [ DEBUG ]  

Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-07 15:58:56  [ main:1044 ] - [ DEBUG ]  
For final plan, using rel#22:LogicalSink.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#21,table=default_catalog.default_database.Unregistered_Collect_Sink_1,fields=product_id, max_price, avg_price)
2023-03-07 15:58:56  [ main:1044 ] - [ DEBUG ]  
For final plan, using rel#20:LogicalProject.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#19,inputs=0,exprs=[MAX($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), /(CASE(>(COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), 0), $SUM0($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), null:DOUBLE), COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING))])
2023-03-07 15:58:56  [ main:1044 ] - [ DEBUG ]  
For final plan, using rel#1:LogicalTableScan.NONE.any.None: 0.[NONE].[NONE](table=[default_catalog, default_database, UnnamedTable$0])
2023-03-07 15:58:56  [ main:1045 ] - [ DEBUG ]  
optimize convert table references after sub-queries removed cost 1 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.Unregistered_Collect_Sink_1], fields=[product_id, max_price, avg_price])
+- LogicalProject(product_id=[$0], max_price=[MAX($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING)], avg_price=[/(CASE(>(COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), 0), $SUM0($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), null:DOUBLE), COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING))])
   +- LogicalTableScan(table=[[default_catalog, default_database, UnnamedTable$0]])

2023-03-07 15:58:56  [ main:1045 ] - [ DEBUG ]  
optimize subquery_rewrite cost 39 ms.
optimize result: 
LogicalSink(table=[default_catalog.default_database.Unregistered_Collect_Sink_1], fields=[product_id, max_price, avg_price])
+- LogicalProject(product_id=[$0], max_price=[MAX($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING)], avg_price=[/(CASE(>(COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), 0), $SUM0($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), null:DOUBLE), COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING))])
   +- LogicalTableScan(table=[[default_catalog, default_database, UnnamedTable$0]])

2023-03-07 15:58:56  [ main:1045 ] - [ DEBUG ]  
iteration: 1
2023-03-07 15:58:56  [ main:1046 ] - [ DEBUG ]  
Rule Attempts Info for HepPlanner
2023-03-07 15:58:56  [ main:1046 ] - [ DEBUG ]  

Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-07 15:58:56  [ main:1046 ] - [ DEBUG ]  
For final plan, using rel#27:LogicalSink.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#26,table=default_catalog.default_database.Unregistered_Collect_Sink_1,fields=product_id, max_price, avg_price)
2023-03-07 15:58:56  [ main:1046 ] - [ DEBUG ]  
For final plan, using rel#25:LogicalProject.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#24,inputs=0,exprs=[MAX($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), /(CASE(>(COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), 0), $SUM0($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), null:DOUBLE), COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING))])
2023-03-07 15:58:56  [ main:1046 ] - [ DEBUG ]  
For final plan, using rel#1:LogicalTableScan.NONE.any.None: 0.[NONE].[NONE](table=[default_catalog, default_database, UnnamedTable$0])
2023-03-07 15:58:56  [ main:1046 ] - [ DEBUG ]  
optimize convert correlate to temporal table join cost 1 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.Unregistered_Collect_Sink_1], fields=[product_id, max_price, avg_price])
+- LogicalProject(product_id=[$0], max_price=[MAX($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING)], avg_price=[/(CASE(>(COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), 0), $SUM0($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), null:DOUBLE), COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING))])
   +- LogicalTableScan(table=[[default_catalog, default_database, UnnamedTable$0]])

2023-03-07 15:58:56  [ main:1047 ] - [ DEBUG ]  
Rule Attempts Info for HepPlanner
2023-03-07 15:58:56  [ main:1047 ] - [ DEBUG ]  

Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-07 15:58:56  [ main:1047 ] - [ DEBUG ]  
For final plan, using rel#32:LogicalSink.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#31,table=default_catalog.default_database.Unregistered_Collect_Sink_1,fields=product_id, max_price, avg_price)
2023-03-07 15:58:56  [ main:1047 ] - [ DEBUG ]  
For final plan, using rel#30:LogicalProject.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#29,inputs=0,exprs=[MAX($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), /(CASE(>(COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), 0), $SUM0($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), null:DOUBLE), COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING))])
2023-03-07 15:58:56  [ main:1047 ] - [ DEBUG ]  
For final plan, using rel#1:LogicalTableScan.NONE.any.None: 0.[NONE].[NONE](table=[default_catalog, default_database, UnnamedTable$0])
2023-03-07 15:58:56  [ main:1047 ] - [ DEBUG ]  
optimize convert enumerable table scan cost 1 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.Unregistered_Collect_Sink_1], fields=[product_id, max_price, avg_price])
+- LogicalProject(product_id=[$0], max_price=[MAX($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING)], avg_price=[/(CASE(>(COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), 0), $SUM0($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), null:DOUBLE), COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING))])
   +- LogicalTableScan(table=[[default_catalog, default_database, UnnamedTable$0]])

2023-03-07 15:58:56  [ main:1047 ] - [ DEBUG ]  
optimize temporal_join_rewrite cost 2 ms.
optimize result: 
LogicalSink(table=[default_catalog.default_database.Unregistered_Collect_Sink_1], fields=[product_id, max_price, avg_price])
+- LogicalProject(product_id=[$0], max_price=[MAX($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING)], avg_price=[/(CASE(>(COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), 0), $SUM0($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), null:DOUBLE), COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING))])
   +- LogicalTableScan(table=[[default_catalog, default_database, UnnamedTable$0]])

2023-03-07 15:58:56  [ main:1048 ] - [ DEBUG ]  
iteration: 1
2023-03-07 15:58:56  [ main:1048 ] - [ DEBUG ]  
Rule Attempts Info for HepPlanner
2023-03-07 15:58:56  [ main:1048 ] - [ DEBUG ]  

Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-07 15:58:56  [ main:1048 ] - [ DEBUG ]  
For final plan, using rel#37:LogicalSink.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#36,table=default_catalog.default_database.Unregistered_Collect_Sink_1,fields=product_id, max_price, avg_price)
2023-03-07 15:58:56  [ main:1048 ] - [ DEBUG ]  
For final plan, using rel#35:LogicalProject.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#34,inputs=0,exprs=[MAX($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), /(CASE(>(COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), 0), $SUM0($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), null:DOUBLE), COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING))])
2023-03-07 15:58:56  [ main:1048 ] - [ DEBUG ]  
For final plan, using rel#1:LogicalTableScan.NONE.any.None: 0.[NONE].[NONE](table=[default_catalog, default_database, UnnamedTable$0])
2023-03-07 15:58:56  [ main:1048 ] - [ DEBUG ]  
optimize pre-rewrite before decorrelation cost 0 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.Unregistered_Collect_Sink_1], fields=[product_id, max_price, avg_price])
+- LogicalProject(product_id=[$0], max_price=[MAX($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING)], avg_price=[/(CASE(>(COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), 0), $SUM0($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), null:DOUBLE), COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING))])
   +- LogicalTableScan(table=[[default_catalog, default_database, UnnamedTable$0]])

2023-03-07 15:58:56  [ main:1060 ] - [ DEBUG ]  
optimize  cost 11 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.Unregistered_Collect_Sink_1], fields=[product_id, max_price, avg_price])
+- LogicalProject(product_id=[$0], max_price=[MAX($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING)], avg_price=[/(CASE(>(COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), 0), $SUM0($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), null:DOUBLE), COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING))])
   +- LogicalTableScan(table=[[default_catalog, default_database, UnnamedTable$0]])

2023-03-07 15:58:56  [ main:1060 ] - [ DEBUG ]  
optimize decorrelate cost 12 ms.
optimize result: 
LogicalSink(table=[default_catalog.default_database.Unregistered_Collect_Sink_1], fields=[product_id, max_price, avg_price])
+- LogicalProject(product_id=[$0], max_price=[MAX($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING)], avg_price=[/(CASE(>(COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), 0), $SUM0($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), null:DOUBLE), COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING))])
   +- LogicalTableScan(table=[[default_catalog, default_database, UnnamedTable$0]])

2023-03-07 15:58:56  [ main:1062 ] - [ DEBUG ]  
call#1: Apply rule [ReduceExpressionsRule(Project)] to [rel#40:LogicalProject.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#39,inputs=0,exprs=[MAX($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), /(CASE(>(COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), 0), $SUM0($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), null:DOUBLE), COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING))])]
2023-03-07 15:58:56  [ main:1079 ] - [ DEBUG ]  
call#2: Apply rule [ProjectToWindowRule:project] to [rel#40:LogicalProject.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#39,inputs=0,exprs=[MAX($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), /(CASE(>(COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), 0), $SUM0($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), null:DOUBLE), COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING))])]
2023-03-07 15:58:56  [ main:1098 ] - [ DEBUG ]  
call#2: Rule ProjectToWindowRule:project arguments [rel#40:LogicalProject.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#39,inputs=0,exprs=[MAX($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), /(CASE(>(COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), 0), $SUM0($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING), null:DOUBLE), COUNT($2) OVER (PARTITION BY $0 ORDER BY $3 NULLS FIRST ROWS 3 PRECEDING))])] produced rel#50:LogicalProject.NONE.any.None: 0.[NONE].[NONE](input=LogicalWindow#47,inputs=0,exprs=[$3, /(CASE(>($4, 0), $5, null:DOUBLE), $4)])
2023-03-07 15:58:56  [ main:1100 ] - [ DEBUG ]  
Rule Attempts Info for HepPlanner
2023-03-07 15:58:56  [ main:1101 ] - [ DEBUG ]  

Rules                                                                   Attempts           Time (us)
ProjectToWindowRule:project                                                    1              19,118
ReduceExpressionsRule(Project)                                                 1              16,541
* Total                                                                        2              35,659

2023-03-07 15:58:56  [ main:1101 ] - [ DEBUG ]  
For final plan, using rel#42:LogicalSink.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#55,table=default_catalog.default_database.Unregistered_Collect_Sink_1,fields=product_id, max_price, avg_price)
2023-03-07 15:58:56  [ main:1101 ] - [ DEBUG ]  
For final plan, using rel#54:LogicalProject.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#53,inputs=0,exprs=[$3, /(CASE(>($4, 0), $5, null:DOUBLE), $4)])
2023-03-07 15:58:56  [ main:1101 ] - [ DEBUG ]  
For final plan, using rel#52:LogicalWindow.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#51,window#0=window(partition {0} order by [2 ASC-nulls-first] rows between $3 PRECEDING and CURRENT ROW aggs [MAX($1), COUNT($1), $SUM0($1)]))
2023-03-07 15:58:56  [ main:1101 ] - [ DEBUG ]  
For final plan, using rel#46:LogicalProject.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#39,inputs=0,exprs=[$2, $3])
2023-03-07 15:58:56  [ main:1102 ] - [ DEBUG ]  
For final plan, using rel#1:LogicalTableScan.NONE.any.None: 0.[NONE].[NONE](table=[default_catalog, default_database, UnnamedTable$0])
2023-03-07 15:58:56  [ main:1102 ] - [ DEBUG ]  
optimize default_rewrite cost 42 ms.
optimize result: 
LogicalSink(table=[default_catalog.default_database.Unregistered_Collect_Sink_1], fields=[product_id, max_price, avg_price])
+- LogicalProject(product_id=[$0], max_price=[$3], avg_price=[/(CASE(>($4, 0), $5, null:DOUBLE), $4)])
   +- LogicalWindow(window#0=[window(partition {0} order by [2 ASC-nulls-first] rows between $3 PRECEDING and CURRENT ROW aggs [MAX($1), COUNT($1), $SUM0($1)])])
      +- LogicalProject(product_id=[$0], price=[$2], date_time=[$3])
         +- LogicalTableScan(table=[[default_catalog, default_database, UnnamedTable$0]])

2023-03-07 15:58:56  [ main:1102 ] - [ DEBUG ]  
iteration: 1
2023-03-07 15:58:56  [ main:1103 ] - [ DEBUG ]  
call#3: Apply rule [ReduceExpressionsRule(Project)] to [rel#57:LogicalProject.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#56,inputs=0,exprs=[$2, $3])]
2023-03-07 15:58:56  [ main:1103 ] - [ DEBUG ]  
call#4: Apply rule [ReduceExpressionsRule(Project)] to [rel#61:LogicalProject.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#60,inputs=0,exprs=[$3, /(CASE(>($4, 0), $5, null:DOUBLE), $4)])]
2023-03-07 15:58:56  [ main:1103 ] - [ DEBUG ]  
Rule Attempts Info for HepPlanner
2023-03-07 15:58:56  [ main:1103 ] - [ DEBUG ]  

Rules                                                                   Attempts           Time (us)
ReduceExpressionsRule(Project)                                                 2                 265
* Total                                                                        2                 265

2023-03-07 15:58:56  [ main:1104 ] - [ DEBUG ]  
For final plan, using rel#63:LogicalSink.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#62,table=default_catalog.default_database.Unregistered_Collect_Sink_1,fields=product_id, max_price, avg_price)
2023-03-07 15:58:56  [ main:1104 ] - [ DEBUG ]  
For final plan, using rel#61:LogicalProject.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#60,inputs=0,exprs=[$3, /(CASE(>($4, 0), $5, null:DOUBLE), $4)])
2023-03-07 15:58:56  [ main:1104 ] - [ DEBUG ]  
For final plan, using rel#59:LogicalWindow.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#58,window#0=window(partition {0} order by [2 ASC-nulls-first] rows between $3 PRECEDING and CURRENT ROW aggs [MAX($1), COUNT($1), $SUM0($1)]))
2023-03-07 15:58:56  [ main:1104 ] - [ DEBUG ]  
For final plan, using rel#57:LogicalProject.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#56,inputs=0,exprs=[$2, $3])
2023-03-07 15:58:56  [ main:1104 ] - [ DEBUG ]  
For final plan, using rel#1:LogicalTableScan.NONE.any.None: 0.[NONE].[NONE](table=[default_catalog, default_database, UnnamedTable$0])
2023-03-07 15:58:56  [ main:1104 ] - [ DEBUG ]  
optimize filter rules cost 2 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.Unregistered_Collect_Sink_1], fields=[product_id, max_price, avg_price])
+- LogicalProject(product_id=[$0], max_price=[$3], avg_price=[/(CASE(>($4, 0), $5, null:DOUBLE), $4)])
   +- LogicalWindow(window#0=[window(partition {0} order by [2 ASC-nulls-first] rows between $3 PRECEDING and CURRENT ROW aggs [MAX($1), COUNT($1), $SUM0($1)])])
      +- LogicalProject(product_id=[$0], price=[$2], date_time=[$3])
         +- LogicalTableScan(table=[[default_catalog, default_database, UnnamedTable$0]])

2023-03-07 15:58:56  [ main:1104 ] - [ DEBUG ]  
Rule Attempts Info for HepPlanner
2023-03-07 15:58:56  [ main:1104 ] - [ DEBUG ]  

Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-07 15:58:56  [ main:1105 ] - [ DEBUG ]  
For final plan, using rel#72:LogicalSink.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#71,table=default_catalog.default_database.Unregistered_Collect_Sink_1,fields=product_id, max_price, avg_price)
2023-03-07 15:58:56  [ main:1105 ] - [ DEBUG ]  
For final plan, using rel#70:LogicalProject.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#69,inputs=0,exprs=[$3, /(CASE(>($4, 0), $5, null:DOUBLE), $4)])
2023-03-07 15:58:56  [ main:1105 ] - [ DEBUG ]  
For final plan, using rel#68:LogicalWindow.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#67,window#0=window(partition {0} order by [2 ASC-nulls-first] rows between $3 PRECEDING and CURRENT ROW aggs [MAX($1), COUNT($1), $SUM0($1)]))
2023-03-07 15:58:56  [ main:1105 ] - [ DEBUG ]  
For final plan, using rel#66:LogicalProject.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#65,inputs=0,exprs=[$2, $3])
2023-03-07 15:58:56  [ main:1105 ] - [ DEBUG ]  
For final plan, using rel#1:LogicalTableScan.NONE.any.None: 0.[NONE].[NONE](table=[default_catalog, default_database, UnnamedTable$0])
2023-03-07 15:58:56  [ main:1105 ] - [ DEBUG ]  
optimize push predicate into table scan cost 1 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.Unregistered_Collect_Sink_1], fields=[product_id, max_price, avg_price])
+- LogicalProject(product_id=[$0], max_price=[$3], avg_price=[/(CASE(>($4, 0), $5, null:DOUBLE), $4)])
   +- LogicalWindow(window#0=[window(partition {0} order by [2 ASC-nulls-first] rows between $3 PRECEDING and CURRENT ROW aggs [MAX($1), COUNT($1), $SUM0($1)])])
      +- LogicalProject(product_id=[$0], price=[$2], date_time=[$3])
         +- LogicalTableScan(table=[[default_catalog, default_database, UnnamedTable$0]])

2023-03-07 15:58:56  [ main:1105 ] - [ DEBUG ]  
Rule Attempts Info for HepPlanner
2023-03-07 15:58:56  [ main:1106 ] - [ DEBUG ]  

Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-07 15:58:56  [ main:1106 ] - [ DEBUG ]  
For final plan, using rel#81:LogicalSink.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#80,table=default_catalog.default_database.Unregistered_Collect_Sink_1,fields=product_id, max_price, avg_price)
2023-03-07 15:58:56  [ main:1106 ] - [ DEBUG ]  
For final plan, using rel#79:LogicalProject.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#78,inputs=0,exprs=[$3, /(CASE(>($4, 0), $5, null:DOUBLE), $4)])
2023-03-07 15:58:56  [ main:1106 ] - [ DEBUG ]  
For final plan, using rel#77:LogicalWindow.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#76,window#0=window(partition {0} order by [2 ASC-nulls-first] rows between $3 PRECEDING and CURRENT ROW aggs [MAX($1), COUNT($1), $SUM0($1)]))
2023-03-07 15:58:56  [ main:1106 ] - [ DEBUG ]  
For final plan, using rel#75:LogicalProject.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#74,inputs=0,exprs=[$2, $3])
2023-03-07 15:58:56  [ main:1106 ] - [ DEBUG ]  
For final plan, using rel#1:LogicalTableScan.NONE.any.None: 0.[NONE].[NONE](table=[default_catalog, default_database, UnnamedTable$0])
2023-03-07 15:58:56  [ main:1107 ] - [ DEBUG ]  
optimize prune empty after predicate push down cost 1 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.Unregistered_Collect_Sink_1], fields=[product_id, max_price, avg_price])
+- LogicalProject(product_id=[$0], max_price=[$3], avg_price=[/(CASE(>($4, 0), $5, null:DOUBLE), $4)])
   +- LogicalWindow(window#0=[window(partition {0} order by [2 ASC-nulls-first] rows between $3 PRECEDING and CURRENT ROW aggs [MAX($1), COUNT($1), $SUM0($1)])])
      +- LogicalProject(product_id=[$0], price=[$2], date_time=[$3])
         +- LogicalTableScan(table=[[default_catalog, default_database, UnnamedTable$0]])

2023-03-07 15:58:56  [ main:1107 ] - [ DEBUG ]  
optimize predicate_pushdown cost 5 ms.
optimize result: 
LogicalSink(table=[default_catalog.default_database.Unregistered_Collect_Sink_1], fields=[product_id, max_price, avg_price])
+- LogicalProject(product_id=[$0], max_price=[$3], avg_price=[/(CASE(>($4, 0), $5, null:DOUBLE), $4)])
   +- LogicalWindow(window#0=[window(partition {0} order by [2 ASC-nulls-first] rows between $3 PRECEDING and CURRENT ROW aggs [MAX($1), COUNT($1), $SUM0($1)])])
      +- LogicalProject(product_id=[$0], price=[$2], date_time=[$3])
         +- LogicalTableScan(table=[[default_catalog, default_database, UnnamedTable$0]])

2023-03-07 15:58:56  [ main:1108 ] - [ DEBUG ]  
Rule Attempts Info for HepPlanner
2023-03-07 15:58:56  [ main:1108 ] - [ DEBUG ]  

Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-07 15:58:56  [ main:1108 ] - [ DEBUG ]  
For final plan, using rel#90:LogicalSink.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#89,table=default_catalog.default_database.Unregistered_Collect_Sink_1,fields=product_id, max_price, avg_price)
2023-03-07 15:58:56  [ main:1108 ] - [ DEBUG ]  
For final plan, using rel#88:LogicalProject.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#87,inputs=0,exprs=[$3, /(CASE(>($4, 0), $5, null:DOUBLE), $4)])
2023-03-07 15:58:56  [ main:1108 ] - [ DEBUG ]  
For final plan, using rel#86:LogicalWindow.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#85,window#0=window(partition {0} order by [2 ASC-nulls-first] rows between $3 PRECEDING and CURRENT ROW aggs [MAX($1), COUNT($1), $SUM0($1)]))
2023-03-07 15:58:56  [ main:1108 ] - [ DEBUG ]  
For final plan, using rel#84:LogicalProject.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#83,inputs=0,exprs=[$2, $3])
2023-03-07 15:58:56  [ main:1108 ] - [ DEBUG ]  
For final plan, using rel#1:LogicalTableScan.NONE.any.None: 0.[NONE].[NONE](table=[default_catalog, default_database, UnnamedTable$0])
2023-03-07 15:58:56  [ main:1109 ] - [ DEBUG ]  
optimize project_rewrite cost 1 ms.
optimize result: 
LogicalSink(table=[default_catalog.default_database.Unregistered_Collect_Sink_1], fields=[product_id, max_price, avg_price])
+- LogicalProject(product_id=[$0], max_price=[$3], avg_price=[/(CASE(>($4, 0), $5, null:DOUBLE), $4)])
   +- LogicalWindow(window#0=[window(partition {0} order by [2 ASC-nulls-first] rows between $3 PRECEDING and CURRENT ROW aggs [MAX($1), COUNT($1), $SUM0($1)])])
      +- LogicalProject(product_id=[$0], price=[$2], date_time=[$3])
         +- LogicalTableScan(table=[[default_catalog, default_database, UnnamedTable$0]])

2023-03-07 15:58:56  [ main:1211 ] - [ DEBUG ]  
PLANNER = org.apache.calcite.plan.volcano.IterativeRuleDriver@4ecd00b5; COST = {inf}
2023-03-07 15:58:56  [ main:1211 ] - [ DEBUG ]  
Pop match: rule [FlinkLogicalDataStreamTableScanConverter(in:NONE,out:LOGICAL)] rels [#1]
2023-03-07 15:58:56  [ main:1211 ] - [ DEBUG ]  
call#17: Apply rule [FlinkLogicalDataStreamTableScanConverter(in:NONE,out:LOGICAL)] to [rel#1:LogicalTableScan.NONE.any.None: 0.[NONE].[NONE](table=[default_catalog, default_database, UnnamedTable$0])]
2023-03-07 15:58:56  [ main:1212 ] - [ DEBUG ]  
Transform to: rel#103 via FlinkLogicalDataStreamTableScanConverter(in:NONE,out:LOGICAL)
2023-03-07 15:58:56  [ main:1250 ] - [ DEBUG ]  
call#17 generated 1 successors: [rel#103:FlinkLogicalDataStreamTableScan.LOGICAL.any.None: 0.[NONE].[NONE](table=[default_catalog, default_database, UnnamedTable$0],fields=product_id, buyer_name, price, date_time)]
2023-03-07 15:58:56  [ main:1250 ] - [ DEBUG ]  
PLANNER = org.apache.calcite.plan.volcano.IterativeRuleDriver@4ecd00b5; COST = {inf}
2023-03-07 15:58:56  [ main:1251 ] - [ DEBUG ]  
Pop match: rule [ProjectToCalcRule] rels [#93]
2023-03-07 15:58:56  [ main:1251 ] - [ DEBUG ]  
call#42: Apply rule [ProjectToCalcRule] to [rel#93:LogicalProject.NONE.any.None: 0.[NONE].[NONE](input=RelSubset#92,inputs=0,exprs=[$2, $3])]
2023-03-07 15:58:56  [ main:1251 ] - [ DEBUG ]  
Transform to: rel#105 via ProjectToCalcRule
2023-03-07 15:58:56  [ main:1252 ] - [ DEBUG ]  
call#42 generated 1 successors: [rel#105:LogicalCalc.NONE.any.None: 0.[NONE].[NONE](input=RelSubset#92,expr#0..3={inputs},0=$t0,1=$t2,2=$t3)]
2023-03-07 15:58:56  [ main:1253 ] - [ DEBUG ]  
PLANNER = org.apache.calcite.plan.volcano.IterativeRuleDriver@4ecd00b5; COST = {inf}
2023-03-07 15:58:56  [ main:1253 ] - [ DEBUG ]  
Pop match: rule [FlinkLogicalOverAggregateConverter(in:NONE,out:LOGICAL)] rels [#95]
2023-03-07 15:58:56  [ main:1253 ] - [ DEBUG ]  
call#50: Apply rule [FlinkLogicalOverAggregateConverter(in:NONE,out:LOGICAL)] to [rel#95:LogicalWindow.NONE.any.None: 0.[NONE].[NONE](input=RelSubset#94,window#0=window(partition {0} order by [2 ASC-nulls-first] rows between $3 PRECEDING and CURRENT ROW aggs [MAX($1), COUNT($1), $SUM0($1)]))]
2023-03-07 15:58:56  [ main:1255 ] - [ DEBUG ]  
Transform to: rel#107 via FlinkLogicalOverAggregateConverter(in:NONE,out:LOGICAL)
2023-03-07 15:58:56  [ main:1265 ] - [ DEBUG ]  
call#50 generated 1 successors: [rel#107:FlinkLogicalOverAggregate.LOGICAL.any.None: 0.[NONE].[NONE](input=RelSubset#106,window#0=window(partition {0} order by [2 ASC-nulls-first] rows between $3 PRECEDING and CURRENT ROW aggs [MAX($1), COUNT($1), $SUM0($1)]))]
2023-03-07 15:58:56  [ main:1265 ] - [ DEBUG ]  
PLANNER = org.apache.calcite.plan.volcano.IterativeRuleDriver@4ecd00b5; COST = {inf}
2023-03-07 15:58:56  [ main:1265 ] - [ DEBUG ]  
Pop match: rule [ProjectToCalcRule] rels [#97]
2023-03-07 15:58:56  [ main:1265 ] - [ DEBUG ]  
call#74: Apply rule [ProjectToCalcRule] to [rel#97:LogicalProject.NONE.any.None: 0.[NONE].[NONE](input=RelSubset#96,inputs=0,exprs=[$3, /(CASE(>($4, 0), $5, null:DOUBLE), $4)])]
2023-03-07 15:58:56  [ main:1266 ] - [ DEBUG ]  
Transform to: rel#109 via ProjectToCalcRule
2023-03-07 15:58:56  [ main:1266 ] - [ DEBUG ]  
call#74 generated 1 successors: [rel#109:LogicalCalc.NONE.any.None: 0.[NONE].[NONE](input=RelSubset#96,expr#0..5={inputs},expr#6=0:BIGINT,expr#7=>($t4, $t6),expr#8=null:DOUBLE,expr#9=CASE($t7, $t5, $t8),expr#10=/($t9, $t4),0=$t0,1=$t3,2=$t10)]
2023-03-07 15:58:56  [ main:1266 ] - [ DEBUG ]  
PLANNER = org.apache.calcite.plan.volcano.IterativeRuleDriver@4ecd00b5; COST = {inf}
2023-03-07 15:58:56  [ main:1266 ] - [ DEBUG ]  
Pop match: rule [FlinkLogicalSinkConverter(in:NONE,out:LOGICAL)] rels [#99]
2023-03-07 15:58:56  [ main:1266 ] - [ DEBUG ]  
call#82: Apply rule [FlinkLogicalSinkConverter(in:NONE,out:LOGICAL)] to [rel#99:LogicalSink.NONE.any.None: 0.[NONE].[NONE](input=RelSubset#98,table=default_catalog.default_database.Unregistered_Collect_Sink_1,fields=product_id, max_price, avg_price)]
2023-03-07 15:58:56  [ main:1266 ] - [ DEBUG ]  
Transform to: rel#111 via FlinkLogicalSinkConverter(in:NONE,out:LOGICAL)
2023-03-07 15:58:56  [ main:1286 ] - [ DEBUG ]  
call#82 generated 1 successors: [rel#111:FlinkLogicalSink.LOGICAL.any.None: 0.[NONE].[NONE](input=RelSubset#110,table=default_catalog.default_database.Unregistered_Collect_Sink_1,fields=product_id, max_price, avg_price)]
2023-03-07 15:58:56  [ main:1286 ] - [ DEBUG ]  
PLANNER = org.apache.calcite.plan.volcano.IterativeRuleDriver@4ecd00b5; COST = {inf}
2023-03-07 15:58:56  [ main:1286 ] - [ DEBUG ]  
Pop match: rule [FlinkLogicalCalcConverter(in:NONE,out:LOGICAL)] rels [#105]
2023-03-07 15:58:56  [ main:1286 ] - [ DEBUG ]  
call#104: Apply rule [FlinkLogicalCalcConverter(in:NONE,out:LOGICAL)] to [rel#105:LogicalCalc.NONE.any.None: 0.[NONE].[NONE](input=RelSubset#92,expr#0..3={inputs},0=$t0,1=$t2,2=$t3)]
2023-03-07 15:58:56  [ main:1287 ] - [ DEBUG ]  
Transform to: rel#112 via FlinkLogicalCalcConverter(in:NONE,out:LOGICAL)
2023-03-07 15:58:56  [ main:1327 ] - [ DEBUG ]  
call#104 generated 1 successors: [rel#112:FlinkLogicalCalc.LOGICAL.any.None: 0.[NONE].[NONE](input=RelSubset#104,select=product_id, price, date_time)]
2023-03-07 15:58:56  [ main:1327 ] - [ DEBUG ]  
PLANNER = org.apache.calcite.plan.volcano.IterativeRuleDriver@4ecd00b5; COST = {inf}
2023-03-07 15:58:56  [ main:1327 ] - [ DEBUG ]  
Pop match: rule [FlinkLogicalCalcConverter(in:NONE,out:LOGICAL)] rels [#109]
2023-03-07 15:58:56  [ main:1327 ] - [ DEBUG ]  
call#121: Apply rule [FlinkLogicalCalcConverter(in:NONE,out:LOGICAL)] to [rel#109:LogicalCalc.NONE.any.None: 0.[NONE].[NONE](input=RelSubset#96,expr#0..5={inputs},expr#6=0:BIGINT,expr#7=>($t4, $t6),expr#8=null:DOUBLE,expr#9=CASE($t7, $t5, $t8),expr#10=/($t9, $t4),0=$t0,1=$t3,2=$t10)]
2023-03-07 15:58:56  [ main:1327 ] - [ DEBUG ]  
Transform to: rel#113 via FlinkLogicalCalcConverter(in:NONE,out:LOGICAL)
2023-03-07 15:58:56  [ main:1329 ] - [ DEBUG ]  
call#121 generated 1 successors: [rel#113:FlinkLogicalCalc.LOGICAL.any.None: 0.[NONE].[NONE](input=RelSubset#108,select=product_id, w0$o0 AS max_price, /(CASE(>(w0$o1, 0:BIGINT), w0$o2, null:DOUBLE), w0$o1) AS avg_price)]
2023-03-07 15:58:56  [ main:1329 ] - [ DEBUG ]  
PLANNER = org.apache.calcite.plan.volcano.IterativeRuleDriver@4ecd00b5; COST = {5.0E8 rows, 7.0E8 cpu, 3.6E9 io, 0.0 network, 0.0 memory}
2023-03-07 15:58:56  [ main:1329 ] - [ DEBUG ]  
Rule Attempts Info for VolcanoPlanner
2023-03-07 15:58:56  [ main:1329 ] - [ DEBUG ]  

Rules                                                                   Attempts           Time (us)
FlinkLogicalCalcConverter(in:NONE,out:LOGICAL)                                 2              42,441
ProjectToCalcRule                                                              2               2,521
FlinkLogicalDataStreamTableScanConverter(in:NONE,out:LOGICAL)                   1              39,101
FlinkLogicalSinkConverter(in:NONE,out:LOGICAL)                                 1              19,938
FlinkLogicalOverAggregateConverter(in:NONE,out:LOGICAL)                        1              12,150
* Total                                                                        7             116,151

2023-03-07 15:58:56  [ main:1353 ] - [ DEBUG ]  
Cheapest plan:
FlinkLogicalSink(table=[default_catalog.default_database.Unregistered_Collect_Sink_1], fields=[product_id, max_price, avg_price]): rowcount = 1.0E8, cumulative cost = {5.0E8 rows, 7.0E8 cpu, 3.6E9 io, 0.0 network, 0.0 memory}, id = 117
  FlinkLogicalCalc(select=[product_id, w0$o0 AS max_price, /(CASE(>(w0$o1, 0:BIGINT), w0$o2, null:DOUBLE), w0$o1) AS avg_price]): rowcount = 1.0E8, cumulative cost = {4.0E8 rows, 6.0E8 cpu, 3.6E9 io, 0.0 network, 0.0 memory}, id = 116
    FlinkLogicalOverAggregate(window#0=[window(partition {0} order by [2 ASC-nulls-first] rows between $3 PRECEDING and CURRENT ROW aggs [MAX($1), COUNT($1), $SUM0($1)])]): rowcount = 1.0E8, cumulative cost = {3.0E8 rows, 5.0E8 cpu, 3.6E9 io, 0.0 network, 0.0 memory}, id = 115
      FlinkLogicalCalc(select=[product_id, price, date_time]): rowcount = 1.0E8, cumulative cost = {2.0E8 rows, 1.0E8 cpu, 3.6E9 io, 0.0 network, 0.0 memory}, id = 114
        FlinkLogicalDataStreamTableScan(table=[[default_catalog, default_database, UnnamedTable$0]], fields=[product_id, buyer_name, price, date_time]): rowcount = 1.0E8, cumulative cost = {1.0E8 rows, 1.0E8 cpu, 3.6E9 io, 0.0 network, 0.0 memory}, id = 103

2023-03-07 15:58:56  [ main:1357 ] - [ DEBUG ]  
Provenance:
rel#117:FlinkLogicalSink.LOGICAL.any.None: 0.[NONE].[NONE](input=FlinkLogicalCalc#116,table=default_catalog.default_database.Unregistered_Collect_Sink_1,fields=product_id, max_price, avg_price)
  direct
    rel#111:FlinkLogicalSink.LOGICAL.any.None: 0.[NONE].[NONE](input=RelSubset#110,table=default_catalog.default_database.Unregistered_Collect_Sink_1,fields=product_id, max_price, avg_price)
      call#82 rule [FlinkLogicalSinkConverter(in:NONE,out:LOGICAL)]
        rel#99:LogicalSink.NONE.any.None: 0.[NONE].[NONE](input=RelSubset#98,table=default_catalog.default_database.Unregistered_Collect_Sink_1,fields=product_id, max_price, avg_price)
          no parent
rel#116:FlinkLogicalCalc.LOGICAL.any.None: 0.[NONE].[NONE](input=FlinkLogicalOverAggregate#115,select=product_id, w0$o0 AS max_price, /(CASE(>(w0$o1, 0:BIGINT), w0$o2, null:DOUBLE), w0$o1) AS avg_price)
  direct
    rel#113:FlinkLogicalCalc.LOGICAL.any.None: 0.[NONE].[NONE](input=RelSubset#108,select=product_id, w0$o0 AS max_price, /(CASE(>(w0$o1, 0:BIGINT), w0$o2, null:DOUBLE), w0$o1) AS avg_price)
      call#121 rule [FlinkLogicalCalcConverter(in:NONE,out:LOGICAL)]
        rel#109:LogicalCalc.NONE.any.None: 0.[NONE].[NONE](input=RelSubset#96,expr#0..5={inputs},expr#6=0:BIGINT,expr#7=>($t4, $t6),expr#8=null:DOUBLE,expr#9=CASE($t7, $t5, $t8),expr#10=/($t9, $t4),0=$t0,1=$t3,2=$t10)
          call#74 rule [ProjectToCalcRule]
            rel#97:LogicalProject.NONE.any.None: 0.[NONE].[NONE](input=RelSubset#96,inputs=0,exprs=[$3, /(CASE(>($4, 0), $5, null:DOUBLE), $4)])
              no parent
rel#115:FlinkLogicalOverAggregate.LOGICAL.any.None: 0.[NONE].[NONE](input=FlinkLogicalCalc#114,window#0=window(partition {0} order by [2 ASC-nulls-first] rows between $3 PRECEDING and CURRENT ROW aggs [MAX($1), COUNT($1), $SUM0($1)]))
  direct
    rel#107:FlinkLogicalOverAggregate.LOGICAL.any.None: 0.[NONE].[NONE](input=RelSubset#106,window#0=window(partition {0} order by [2 ASC-nulls-first] rows between $3 PRECEDING and CURRENT ROW aggs [MAX($1), COUNT($1), $SUM0($1)]))
      call#50 rule [FlinkLogicalOverAggregateConverter(in:NONE,out:LOGICAL)]
        rel#95:LogicalWindow.NONE.any.None: 0.[NONE].[NONE](input=RelSubset#94,window#0=window(partition {0} order by [2 ASC-nulls-first] rows between $3 PRECEDING and CURRENT ROW aggs [MAX($1), COUNT($1), $SUM0($1)]))
          no parent
rel#114:FlinkLogicalCalc.LOGICAL.any.None: 0.[NONE].[NONE](input=FlinkLogicalDataStreamTableScan#103,select=product_id, price, date_time)
  direct
    rel#112:FlinkLogicalCalc.LOGICAL.any.None: 0.[NONE].[NONE](input=RelSubset#104,select=product_id, price, date_time)
      call#104 rule [FlinkLogicalCalcConverter(in:NONE,out:LOGICAL)]
        rel#105:LogicalCalc.NONE.any.None: 0.[NONE].[NONE](input=RelSubset#92,expr#0..3={inputs},0=$t0,1=$t2,2=$t3)
          call#42 rule [ProjectToCalcRule]
            rel#93:LogicalProject.NONE.any.None: 0.[NONE].[NONE](input=RelSubset#92,inputs=0,exprs=[$2, $3])
              no parent
rel#103:FlinkLogicalDataStreamTableScan.LOGICAL.any.None: 0.[NONE].[NONE](table=[default_catalog, default_database, UnnamedTable$0],fields=product_id, buyer_name, price, date_time)
  call#17 rule [FlinkLogicalDataStreamTableScanConverter(in:NONE,out:LOGICAL)]
    rel#1:LogicalTableScan.NONE.any.None: 0.[NONE].[NONE](table=[default_catalog, default_database, UnnamedTable$0])
      no parent

2023-03-07 15:58:56  [ main:1358 ] - [ DEBUG ]  
optimize logical cost 249 ms.
optimize result: 
FlinkLogicalSink(table=[default_catalog.default_database.Unregistered_Collect_Sink_1], fields=[product_id, max_price, avg_price])
+- FlinkLogicalCalc(select=[product_id, w0$o0 AS max_price, /(CASE(>(w0$o1, 0:BIGINT), w0$o2, null:DOUBLE), w0$o1) AS avg_price])
   +- FlinkLogicalOverAggregate(window#0=[window(partition {0} order by [2 ASC-nulls-first] rows between $3 PRECEDING and CURRENT ROW aggs [MAX($1), COUNT($1), $SUM0($1)])])
      +- FlinkLogicalCalc(select=[product_id, price, date_time])
         +- FlinkLogicalDataStreamTableScan(table=[[default_catalog, default_database, UnnamedTable$0]], fields=[product_id, buyer_name, price, date_time])

2023-03-07 15:58:56  [ main:1368 ] - [ DEBUG ]  
Rule Attempts Info for HepPlanner
2023-03-07 15:58:56  [ main:1369 ] - [ DEBUG ]  

Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-07 15:58:56  [ main:1369 ] - [ DEBUG ]  
For final plan, using rel#125:FlinkLogicalSink.LOGICAL.any.None: 0.[NONE].[NONE](input=HepRelVertex#124,table=default_catalog.default_database.Unregistered_Collect_Sink_1,fields=product_id, max_price, avg_price)
2023-03-07 15:58:56  [ main:1369 ] - [ DEBUG ]  
For final plan, using rel#123:FlinkLogicalCalc.LOGICAL.any.None: 0.[NONE].[NONE](input=HepRelVertex#122,select=product_id, w0$o0 AS max_price, /(CASE(>(w0$o1, 0:BIGINT), w0$o2, null:DOUBLE), w0$o1) AS avg_price)
2023-03-07 15:58:56  [ main:1369 ] - [ DEBUG ]  
For final plan, using rel#121:FlinkLogicalOverAggregate.LOGICAL.any.None: 0.[NONE].[NONE](input=HepRelVertex#120,window#0=window(partition {0} order by [2 ASC-nulls-first] rows between $3 PRECEDING and CURRENT ROW aggs [MAX($1), COUNT($1), $SUM0($1)]))
2023-03-07 15:58:56  [ main:1369 ] - [ DEBUG ]  
For final plan, using rel#119:FlinkLogicalCalc.LOGICAL.any.None: 0.[NONE].[NONE](input=HepRelVertex#118,select=product_id, price, date_time)
2023-03-07 15:58:56  [ main:1369 ] - [ DEBUG ]  
For final plan, using rel#103:FlinkLogicalDataStreamTableScan.LOGICAL.any.None: 0.[NONE].[NONE](table=[default_catalog, default_database, UnnamedTable$0],fields=product_id, buyer_name, price, date_time)
2023-03-07 15:58:56  [ main:1370 ] - [ DEBUG ]  
optimize logical_rewrite cost 11 ms.
optimize result: 
FlinkLogicalSink(table=[default_catalog.default_database.Unregistered_Collect_Sink_1], fields=[product_id, max_price, avg_price])
+- FlinkLogicalCalc(select=[product_id, w0$o0 AS max_price, /(CASE(>(w0$o1, 0:BIGINT), w0$o2, null:DOUBLE), w0$o1) AS avg_price])
   +- FlinkLogicalOverAggregate(window#0=[window(partition {0} order by [2 ASC-nulls-first] rows between $3 PRECEDING and CURRENT ROW aggs [MAX($1), COUNT($1), $SUM0($1)])])
      +- FlinkLogicalCalc(select=[product_id, price, date_time])
         +- FlinkLogicalDataStreamTableScan(table=[[default_catalog, default_database, UnnamedTable$0]], fields=[product_id, buyer_name, price, date_time])

2023-03-07 15:58:56  [ main:1380 ] - [ DEBUG ]  
optimize time_indicator cost 9 ms.
optimize result: 
FlinkLogicalSink(table=[default_catalog.default_database.Unregistered_Collect_Sink_1], fields=[product_id, max_price, avg_price])
+- FlinkLogicalCalc(select=[product_id, w0$o0 AS max_price, /(CASE(>(w0$o1, 0:BIGINT), w0$o2, null:DOUBLE), w0$o1) AS avg_price])
   +- FlinkLogicalOverAggregate(window#0=[window(partition {0} order by [2 ASC-nulls-first] rows between $3 PRECEDING and CURRENT ROW aggs [MAX($1), COUNT($1), $SUM0($1)])])
      +- FlinkLogicalCalc(select=[product_id, price, date_time])
         +- FlinkLogicalDataStreamTableScan(table=[[default_catalog, default_database, UnnamedTable$0]], fields=[product_id, buyer_name, price, date_time])

2023-03-07 15:58:56  [ main:1393 ] - [ DEBUG ]  
PLANNER = org.apache.calcite.plan.volcano.IterativeRuleDriver@4ecd00b5; COST = {inf}
2023-03-07 15:58:56  [ main:1393 ] - [ DEBUG ]  
Pop match: rule [StreamPhysicalDataStreamScanRule(in:LOGICAL,out:STREAM_PHYSICAL)] rels [#103]
2023-03-07 15:58:56  [ main:1393 ] - [ DEBUG ]  
call#162: Apply rule [StreamPhysicalDataStreamScanRule(in:LOGICAL,out:STREAM_PHYSICAL)] to [rel#103:FlinkLogicalDataStreamTableScan.LOGICAL.any.None: 0.[NONE].[NONE](table=[default_catalog, default_database, UnnamedTable$0],fields=product_id, buyer_name, price, date_time)]
2023-03-07 15:58:56  [ main:1395 ] - [ DEBUG ]  
Transform to: rel#142 via StreamPhysicalDataStreamScanRule(in:LOGICAL,out:STREAM_PHYSICAL)
2023-03-07 15:58:56  [ main:1424 ] - [ DEBUG ]  
call#162 generated 1 successors: [rel#142:StreamPhysicalDataStreamScan.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE](table=[default_catalog, default_database, UnnamedTable$0],fields=product_id, buyer_name, price, date_time)]
2023-03-07 15:58:56  [ main:1425 ] - [ DEBUG ]  
PLANNER = org.apache.calcite.plan.volcano.IterativeRuleDriver@4ecd00b5; COST = {inf}
2023-03-07 15:58:56  [ main:1425 ] - [ DEBUG ]  
Pop match: rule [StreamPhysicalCalcRule(in:LOGICAL,out:STREAM_PHYSICAL)] rels [#132]
2023-03-07 15:58:56  [ main:1425 ] - [ DEBUG ]  
call#177: Apply rule [StreamPhysicalCalcRule(in:LOGICAL,out:STREAM_PHYSICAL)] to [rel#132:FlinkLogicalCalc.LOGICAL.any.None: 0.[NONE].[NONE](input=RelSubset#131,select=product_id, price, date_time)]
2023-03-07 15:58:56  [ main:1425 ] - [ DEBUG ]  
Transform to: rel#144 via StreamPhysicalCalcRule(in:LOGICAL,out:STREAM_PHYSICAL)
2023-03-07 15:58:56  [ main:1451 ] - [ DEBUG ]  
call#177 generated 1 successors: [rel#144:StreamPhysicalCalc.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE](input=RelSubset#143,select=product_id, price, date_time)]
2023-03-07 15:58:56  [ main:1451 ] - [ DEBUG ]  
PLANNER = org.apache.calcite.plan.volcano.IterativeRuleDriver@4ecd00b5; COST = {inf}
2023-03-07 15:58:56  [ main:1451 ] - [ DEBUG ]  
Pop match: rule [StreamPhysicalOverAggregateRule(in:LOGICAL,out:STREAM_PHYSICAL)] rels [#134]
2023-03-07 15:58:56  [ main:1451 ] - [ DEBUG ]  
call#192: Apply rule [StreamPhysicalOverAggregateRule(in:LOGICAL,out:STREAM_PHYSICAL)] to [rel#134:FlinkLogicalOverAggregate.LOGICAL.any.None: 0.[NONE].[NONE](input=RelSubset#133,window#0=window(partition {0} order by [2 ASC-nulls-first] rows between $3 PRECEDING and CURRENT ROW aggs [MAX($1), COUNT($1), $SUM0($1)]))]
2023-03-07 15:58:56  [ main:1453 ] - [ DEBUG ]  
Transform to: rel#148 via StreamPhysicalOverAggregateRule(in:LOGICAL,out:STREAM_PHYSICAL)
2023-03-07 15:58:56  [ main:1473 ] - [ DEBUG ]  
call#192 generated 1 successors: [rel#148:StreamPhysicalOverAggregate.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE](input=RelSubset#146,partitionBy=product_id,orderBy=date_time ASC,window= ROWS BETWEEN 3 PRECEDING AND CURRENT ROW,select=product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2)]
2023-03-07 15:58:56  [ main:1473 ] - [ DEBUG ]  
PLANNER = org.apache.calcite.plan.volcano.IterativeRuleDriver@4ecd00b5; COST = {inf}
2023-03-07 15:58:56  [ main:1473 ] - [ DEBUG ]  
Pop match: rule [StreamPhysicalCalcRule(in:LOGICAL,out:STREAM_PHYSICAL)] rels [#136]
2023-03-07 15:58:56  [ main:1474 ] - [ DEBUG ]  
call#206: Apply rule [StreamPhysicalCalcRule(in:LOGICAL,out:STREAM_PHYSICAL)] to [rel#136:FlinkLogicalCalc.LOGICAL.any.None: 0.[NONE].[NONE](input=RelSubset#135,select=product_id, w0$o0 AS max_price, /(CASE(>(w0$o1, 0:BIGINT), w0$o2, null:DOUBLE), w0$o1) AS avg_price)]
2023-03-07 15:58:56  [ main:1474 ] - [ DEBUG ]  
Transform to: rel#150 via StreamPhysicalCalcRule(in:LOGICAL,out:STREAM_PHYSICAL)
2023-03-07 15:58:56  [ main:1475 ] - [ DEBUG ]  
call#206 generated 1 successors: [rel#150:StreamPhysicalCalc.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE](input=RelSubset#149,select=product_id, w0$o0 AS max_price, /(CASE(>(w0$o1, 0:BIGINT), w0$o2, null:DOUBLE), w0$o1) AS avg_price)]
2023-03-07 15:58:56  [ main:1475 ] - [ DEBUG ]  
PLANNER = org.apache.calcite.plan.volcano.IterativeRuleDriver@4ecd00b5; COST = {inf}
2023-03-07 15:58:56  [ main:1475 ] - [ DEBUG ]  
Pop match: rule [StreamPhysicalSinkRule(in:LOGICAL,out:STREAM_PHYSICAL)] rels [#138]
2023-03-07 15:58:56  [ main:1475 ] - [ DEBUG ]  
call#231: Apply rule [StreamPhysicalSinkRule(in:LOGICAL,out:STREAM_PHYSICAL)] to [rel#138:FlinkLogicalSink.LOGICAL.any.None: 0.[NONE].[NONE](input=RelSubset#137,table=default_catalog.default_database.Unregistered_Collect_Sink_1,fields=product_id, max_price, avg_price)]
2023-03-07 15:58:56  [ main:1476 ] - [ DEBUG ]  
Transform to: rel#152 via StreamPhysicalSinkRule(in:LOGICAL,out:STREAM_PHYSICAL)
2023-03-07 15:58:56  [ main:1492 ] - [ DEBUG ]  
call#231 generated 1 successors: [rel#152:StreamPhysicalSink.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE](input=RelSubset#151,table=default_catalog.default_database.Unregistered_Collect_Sink_1,fields=product_id, max_price, avg_price)]
2023-03-07 15:58:56  [ main:1492 ] - [ DEBUG ]  
PLANNER = org.apache.calcite.plan.volcano.IterativeRuleDriver@4ecd00b5; COST = {inf}
2023-03-07 15:58:56  [ main:1493 ] - [ DEBUG ]  
Pop match: rule [FlinkExpandConversionRule] rels [#147,#144]
2023-03-07 15:58:56  [ main:1493 ] - [ DEBUG ]  
call#243: Apply rule [FlinkExpandConversionRule] to [rel#147:AbstractConverter.STREAM_PHYSICAL.hash[0]true.None: 0.[NONE].[NONE](input=RelSubset#145,convention=STREAM_PHYSICAL,FlinkRelDistributionTraitDef=hash[0]true,MiniBatchIntervalTraitDef=None: 0,ModifyKindSetTraitDef=[NONE],UpdateKindTraitDef=[NONE]), rel#144:StreamPhysicalCalc.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE](input=RelSubset#143,select=product_id, price, date_time)]
2023-03-07 15:58:56  [ main:1494 ] - [ DEBUG ]  
Transform to: rel#153 via FlinkExpandConversionRule
2023-03-07 15:58:56  [ main:1534 ] - [ DEBUG ]  
call#243 generated 1 successors: [rel#153:StreamPhysicalExchange.STREAM_PHYSICAL.hash[0]true.None: 0.[NONE].[NONE](input=StreamPhysicalCalc#144,distribution=hash[product_id])]
2023-03-07 15:58:56  [ main:1534 ] - [ DEBUG ]  
PLANNER = org.apache.calcite.plan.volcano.IterativeRuleDriver@4ecd00b5; COST = {6.0E8 rows, 1.7266666666666664E10 cpu, 3.6E9 io, 2.4E9 network, 0.0 memory}
2023-03-07 15:58:56  [ main:1534 ] - [ DEBUG ]  
Rule Attempts Info for VolcanoPlanner
2023-03-07 15:58:56  [ main:1535 ] - [ DEBUG ]  

Rules                                                                   Attempts           Time (us)
FlinkLogicalCalcConverter(in:NONE,out:LOGICAL)                                 2              42,441
StreamPhysicalCalcRule(in:LOGICAL,out:STREAM_PHYSICAL)                         2              26,747
ProjectToCalcRule                                                              2               2,521
FlinkExpandConversionRule                                                      1              41,427
FlinkLogicalDataStreamTableScanConverter(in:NONE,out:LOGICAL)                   1              39,101
StreamPhysicalDataStreamScanRule(in:LOGICAL,out:STREAM_PHYSICAL)                   1              31,885
StreamPhysicalOverAggregateRule(in:LOGICAL,out:STREAM_PHYSICAL)                   1              22,539
FlinkLogicalSinkConverter(in:NONE,out:LOGICAL)                                 1              19,938
StreamPhysicalSinkRule(in:LOGICAL,out:STREAM_PHYSICAL)                         1              17,568
FlinkLogicalOverAggregateConverter(in:NONE,out:LOGICAL)                        1              12,150
* Total                                                                       13             256,317

2023-03-07 15:58:56  [ main:1549 ] - [ DEBUG ]  
Cheapest plan:
StreamPhysicalSink(table=[default_catalog.default_database.Unregistered_Collect_Sink_1], fields=[product_id, max_price, avg_price]): rowcount = 1.0E8, cumulative cost = {6.0E8 rows, 1.7266666666666664E10 cpu, 3.6E9 io, 2.4E9 network, 0.0 memory}, id = 159
  StreamPhysicalCalc(select=[product_id, w0$o0 AS max_price, /(CASE(>(w0$o1, 0:BIGINT), w0$o2, null:DOUBLE), w0$o1) AS avg_price]): rowcount = 1.0E8, cumulative cost = {5.0E8 rows, 1.7166666666666666E10 cpu, 3.6E9 io, 2.4E9 network, 0.0 memory}, id = 158
    StreamPhysicalOverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]): rowcount = 1.0E8, cumulative cost = {4.0E8 rows, 1.7066666666666666E10 cpu, 3.6E9 io, 2.4E9 network, 0.0 memory}, id = 157
      StreamPhysicalExchange(distribution=[hash[product_id]]): rowcount = 1.0E8, cumulative cost = {3.0E8 rows, 1.69E10 cpu, 3.6E9 io, 2.4E9 network, 0.0 memory}, id = 156
        StreamPhysicalCalc(select=[product_id, price, date_time]): rowcount = 1.0E8, cumulative cost = {2.0E8 rows, 1.0E8 cpu, 3.6E9 io, 0.0 network, 0.0 memory}, id = 155
          StreamPhysicalDataStreamScan(table=[[default_catalog, default_database, UnnamedTable$0]], fields=[product_id, buyer_name, price, date_time]): rowcount = 1.0E8, cumulative cost = {1.0E8 rows, 1.0E8 cpu, 3.6E9 io, 0.0 network, 0.0 memory}, id = 142

2023-03-07 15:58:56  [ main:1550 ] - [ DEBUG ]  
Provenance:
rel#159:StreamPhysicalSink.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE](input=StreamPhysicalCalc#158,table=default_catalog.default_database.Unregistered_Collect_Sink_1,fields=product_id, max_price, avg_price)
  direct
    rel#152:StreamPhysicalSink.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE](input=RelSubset#151,table=default_catalog.default_database.Unregistered_Collect_Sink_1,fields=product_id, max_price, avg_price)
      call#231 rule [StreamPhysicalSinkRule(in:LOGICAL,out:STREAM_PHYSICAL)]
        rel#138:FlinkLogicalSink.LOGICAL.any.None: 0.[NONE].[NONE](input=RelSubset#137,table=default_catalog.default_database.Unregistered_Collect_Sink_1,fields=product_id, max_price, avg_price)
          no parent
rel#158:StreamPhysicalCalc.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE](input=StreamPhysicalOverAggregate#157,select=product_id, w0$o0 AS max_price, /(CASE(>(w0$o1, 0:BIGINT), w0$o2, null:DOUBLE), w0$o1) AS avg_price)
  direct
    rel#150:StreamPhysicalCalc.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE](input=RelSubset#149,select=product_id, w0$o0 AS max_price, /(CASE(>(w0$o1, 0:BIGINT), w0$o2, null:DOUBLE), w0$o1) AS avg_price)
      call#206 rule [StreamPhysicalCalcRule(in:LOGICAL,out:STREAM_PHYSICAL)]
        rel#136:FlinkLogicalCalc.LOGICAL.any.None: 0.[NONE].[NONE](input=RelSubset#135,select=product_id, w0$o0 AS max_price, /(CASE(>(w0$o1, 0:BIGINT), w0$o2, null:DOUBLE), w0$o1) AS avg_price)
          no parent
rel#157:StreamPhysicalOverAggregate.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE](input=StreamPhysicalExchange#156,partitionBy=product_id,orderBy=date_time ASC,window= ROWS BETWEEN 3 PRECEDING AND CURRENT ROW,select=product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2)
  direct
    rel#148:StreamPhysicalOverAggregate.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE](input=RelSubset#146,partitionBy=product_id,orderBy=date_time ASC,window= ROWS BETWEEN 3 PRECEDING AND CURRENT ROW,select=product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2)
      call#192 rule [StreamPhysicalOverAggregateRule(in:LOGICAL,out:STREAM_PHYSICAL)]
        rel#134:FlinkLogicalOverAggregate.LOGICAL.any.None: 0.[NONE].[NONE](input=RelSubset#133,window#0=window(partition {0} order by [2 ASC-nulls-first] rows between $3 PRECEDING and CURRENT ROW aggs [MAX($1), COUNT($1), $SUM0($1)]))
          no parent
rel#156:StreamPhysicalExchange.STREAM_PHYSICAL.hash[0]true.None: 0.[NONE].[NONE](input=StreamPhysicalCalc#155,distribution=hash[product_id])
  direct
    rel#154:StreamPhysicalExchange.STREAM_PHYSICAL.hash[0]true.None: 0.[NONE].[NONE](input=RelSubset#145,distribution=hash[product_id])
      call#243 rule [FlinkExpandConversionRule]
        rel#147:AbstractConverter.STREAM_PHYSICAL.hash[0]true.None: 0.[NONE].[NONE](input=RelSubset#145,convention=STREAM_PHYSICAL,FlinkRelDistributionTraitDef=hash[0]true,MiniBatchIntervalTraitDef=None: 0,ModifyKindSetTraitDef=[NONE],UpdateKindTraitDef=[NONE])
          call#192 rule [StreamPhysicalOverAggregateRule(in:LOGICAL,out:STREAM_PHYSICAL)]
            rel#134 (see above)
        rel#144:StreamPhysicalCalc.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE](input=RelSubset#143,select=product_id, price, date_time)
          call#177 rule [StreamPhysicalCalcRule(in:LOGICAL,out:STREAM_PHYSICAL)]
            rel#132:FlinkLogicalCalc.LOGICAL.any.None: 0.[NONE].[NONE](input=RelSubset#131,select=product_id, price, date_time)
              no parent
rel#155:StreamPhysicalCalc.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE](input=StreamPhysicalDataStreamScan#142,select=product_id, price, date_time)
  direct
    rel#144 (see above)
rel#142:StreamPhysicalDataStreamScan.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE](table=[default_catalog, default_database, UnnamedTable$0],fields=product_id, buyer_name, price, date_time)
  call#162 rule [StreamPhysicalDataStreamScanRule(in:LOGICAL,out:STREAM_PHYSICAL)]
    rel#103:FlinkLogicalDataStreamTableScan.LOGICAL.any.None: 0.[NONE].[NONE](table=[default_catalog, default_database, UnnamedTable$0],fields=product_id, buyer_name, price, date_time)
      no parent

2023-03-07 15:58:56  [ main:1551 ] - [ DEBUG ]  
optimize physical cost 170 ms.
optimize result: 
Sink(table=[default_catalog.default_database.Unregistered_Collect_Sink_1], fields=[product_id, max_price, avg_price])
+- Calc(select=[product_id, w0$o0 AS max_price, /(CASE(>(w0$o1, 0:BIGINT), w0$o2, null:DOUBLE), w0$o1) AS avg_price])
   +- OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2])
      +- Exchange(distribution=[hash[product_id]])
         +- Calc(select=[product_id, price, date_time])
            +- DataStreamScan(table=[[default_catalog, default_database, UnnamedTable$0]], fields=[product_id, buyer_name, price, date_time])

2023-03-07 15:58:56  [ main:1551 ] - [ DEBUG ]  
iteration: 1
2023-03-07 15:58:56  [ main:1551 ] - [ DEBUG ]  
Rule Attempts Info for HepPlanner
2023-03-07 15:58:56  [ main:1552 ] - [ DEBUG ]  

Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-07 15:58:56  [ main:1552 ] - [ DEBUG ]  
For final plan, using rel#169:StreamPhysicalSink.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE](input=HepRelVertex#168,table=default_catalog.default_database.Unregistered_Collect_Sink_1,fields=product_id, max_price, avg_price)
2023-03-07 15:58:56  [ main:1552 ] - [ DEBUG ]  
For final plan, using rel#167:StreamPhysicalCalc.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE](input=HepRelVertex#166,select=product_id, w0$o0 AS max_price, /(CASE(>(w0$o1, 0:BIGINT), w0$o2, null:DOUBLE), w0$o1) AS avg_price)
2023-03-07 15:58:56  [ main:1552 ] - [ DEBUG ]  
For final plan, using rel#165:StreamPhysicalOverAggregate.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE](input=HepRelVertex#164,partitionBy=product_id,orderBy=date_time ASC,window= ROWS BETWEEN 3 PRECEDING AND CURRENT ROW,select=product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2)
2023-03-07 15:58:56  [ main:1552 ] - [ DEBUG ]  
For final plan, using rel#163:StreamPhysicalExchange.STREAM_PHYSICAL.hash[0]true.None: 0.[NONE].[NONE](input=HepRelVertex#162,distribution=hash[product_id])
2023-03-07 15:58:56  [ main:1552 ] - [ DEBUG ]  
For final plan, using rel#161:StreamPhysicalCalc.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE](input=HepRelVertex#160,select=product_id, price, date_time)
2023-03-07 15:58:56  [ main:1553 ] - [ DEBUG ]  
For final plan, using rel#142:StreamPhysicalDataStreamScan.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE](table=[default_catalog, default_database, UnnamedTable$0],fields=product_id, buyer_name, price, date_time)
2023-03-07 15:58:56  [ main:1554 ] - [ DEBUG ]  
optimize watermark transpose cost 2 ms.
optimize result:
 Sink(table=[default_catalog.default_database.Unregistered_Collect_Sink_1], fields=[product_id, max_price, avg_price])
+- Calc(select=[product_id, w0$o0 AS max_price, /(CASE(>(w0$o1, 0:BIGINT), w0$o2, null:DOUBLE), w0$o1) AS avg_price])
   +- OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2])
      +- Exchange(distribution=[hash[product_id]])
         +- Calc(select=[product_id, price, date_time])
            +- DataStreamScan(table=[[default_catalog, default_database, UnnamedTable$0]], fields=[product_id, buyer_name, price, date_time])

2023-03-07 15:58:56  [ main:1563 ] - [ DEBUG ]  
optimize Changelog mode inference cost 8 ms.
optimize result:
 Sink(table=[default_catalog.default_database.Unregistered_Collect_Sink_1], fields=[product_id, max_price, avg_price])
+- Calc(select=[product_id, w0$o0 AS max_price, /(CASE(>(w0$o1, 0:BIGINT), w0$o2, null:DOUBLE), w0$o1) AS avg_price])
   +- OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2])
      +- Exchange(distribution=[hash[product_id]])
         +- Calc(select=[product_id, price, date_time])
            +- DataStreamScan(table=[[default_catalog, default_database, UnnamedTable$0]], fields=[product_id, buyer_name, price, date_time])

2023-03-07 15:58:56  [ main:1564 ] - [ DEBUG ]  
optimize Initialization for mini-batch interval inference cost 0 ms.
optimize result:
 Sink(table=[default_catalog.default_database.Unregistered_Collect_Sink_1], fields=[product_id, max_price, avg_price])
+- Calc(select=[product_id, w0$o0 AS max_price, /(CASE(>(w0$o1, 0:BIGINT), w0$o2, null:DOUBLE), w0$o1) AS avg_price])
   +- OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2])
      +- Exchange(distribution=[hash[product_id]])
         +- Calc(select=[product_id, price, date_time])
            +- DataStreamScan(table=[[default_catalog, default_database, UnnamedTable$0]], fields=[product_id, buyer_name, price, date_time])

2023-03-07 15:58:56  [ main:1564 ] - [ DEBUG ]  
call#254: Apply rule [MiniBatchIntervalInferRule] to [rel#198:StreamPhysicalSink.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE](input=HepRelVertex#197,table=default_catalog.default_database.Unregistered_Collect_Sink_1,fields=product_id, max_price, avg_price)]
2023-03-07 15:58:56  [ main:1566 ] - [ DEBUG ]  
call#255: Apply rule [MiniBatchIntervalInferRule] to [rel#196:StreamPhysicalCalc.STREAM_PHYSICAL.any.None: 0.[I].[NONE](input=HepRelVertex#195,select=product_id, w0$o0 AS max_price, /(CASE(>(w0$o1, 0:BIGINT), w0$o2, null:DOUBLE), w0$o1) AS avg_price)]
2023-03-07 15:58:56  [ main:1566 ] - [ DEBUG ]  
call#256: Apply rule [MiniBatchIntervalInferRule] to [rel#194:StreamPhysicalOverAggregate.STREAM_PHYSICAL.any.None: 0.[I].[NONE](input=HepRelVertex#193,partitionBy=product_id,orderBy=date_time ASC,window= ROWS BETWEEN 3 PRECEDING AND CURRENT ROW,select=product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2)]
2023-03-07 15:58:56  [ main:1567 ] - [ DEBUG ]  
call#257: Apply rule [MiniBatchIntervalInferRule] to [rel#192:StreamPhysicalExchange.STREAM_PHYSICAL.hash[0]true.None: 0.[I].[NONE](input=HepRelVertex#191,distribution=hash[product_id])]
2023-03-07 15:58:56  [ main:1567 ] - [ DEBUG ]  
call#258: Apply rule [MiniBatchIntervalInferRule] to [rel#190:StreamPhysicalCalc.STREAM_PHYSICAL.any.None: 0.[I].[NONE](input=HepRelVertex#189,select=product_id, price, date_time)]
2023-03-07 15:58:56  [ main:1567 ] - [ DEBUG ]  
call#259: Apply rule [MiniBatchIntervalInferRule] to [rel#182:StreamPhysicalDataStreamScan.STREAM_PHYSICAL.any.None: 0.[I].[NONE](table=[default_catalog, default_database, UnnamedTable$0],fields=product_id, buyer_name, price, date_time)]
2023-03-07 15:58:56  [ main:1567 ] - [ DEBUG ]  
Rule Attempts Info for HepPlanner
2023-03-07 15:58:56  [ main:1567 ] - [ DEBUG ]  

Rules                                                                   Attempts           Time (us)
MiniBatchIntervalInferRule                                                     6               1,659
* Total                                                                        6               1,659

2023-03-07 15:58:56  [ main:1567 ] - [ DEBUG ]  
For final plan, using rel#198:StreamPhysicalSink.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE](input=HepRelVertex#197,table=default_catalog.default_database.Unregistered_Collect_Sink_1,fields=product_id, max_price, avg_price)
2023-03-07 15:58:56  [ main:1567 ] - [ DEBUG ]  
For final plan, using rel#196:StreamPhysicalCalc.STREAM_PHYSICAL.any.None: 0.[I].[NONE](input=HepRelVertex#195,select=product_id, w0$o0 AS max_price, /(CASE(>(w0$o1, 0:BIGINT), w0$o2, null:DOUBLE), w0$o1) AS avg_price)
2023-03-07 15:58:56  [ main:1568 ] - [ DEBUG ]  
For final plan, using rel#194:StreamPhysicalOverAggregate.STREAM_PHYSICAL.any.None: 0.[I].[NONE](input=HepRelVertex#193,partitionBy=product_id,orderBy=date_time ASC,window= ROWS BETWEEN 3 PRECEDING AND CURRENT ROW,select=product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2)
2023-03-07 15:58:56  [ main:1568 ] - [ DEBUG ]  
For final plan, using rel#192:StreamPhysicalExchange.STREAM_PHYSICAL.hash[0]true.None: 0.[I].[NONE](input=HepRelVertex#191,distribution=hash[product_id])
2023-03-07 15:58:56  [ main:1568 ] - [ DEBUG ]  
For final plan, using rel#190:StreamPhysicalCalc.STREAM_PHYSICAL.any.None: 0.[I].[NONE](input=HepRelVertex#189,select=product_id, price, date_time)
2023-03-07 15:58:56  [ main:1568 ] - [ DEBUG ]  
For final plan, using rel#182:StreamPhysicalDataStreamScan.STREAM_PHYSICAL.any.None: 0.[I].[NONE](table=[default_catalog, default_database, UnnamedTable$0],fields=product_id, buyer_name, price, date_time)
2023-03-07 15:58:56  [ main:1569 ] - [ DEBUG ]  
optimize mini-batch interval rules cost 4 ms.
optimize result:
 Sink(table=[default_catalog.default_database.Unregistered_Collect_Sink_1], fields=[product_id, max_price, avg_price])
+- Calc(select=[product_id, w0$o0 AS max_price, /(CASE(>(w0$o1, 0:BIGINT), w0$o2, null:DOUBLE), w0$o1) AS avg_price])
   +- OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2])
      +- Exchange(distribution=[hash[product_id]])
         +- Calc(select=[product_id, price, date_time])
            +- DataStreamScan(table=[[default_catalog, default_database, UnnamedTable$0]], fields=[product_id, buyer_name, price, date_time])

2023-03-07 15:58:56  [ main:1570 ] - [ DEBUG ]  
Rule Attempts Info for HepPlanner
2023-03-07 15:58:56  [ main:1570 ] - [ DEBUG ]  

Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-07 15:58:56  [ main:1570 ] - [ DEBUG ]  
For final plan, using rel#209:StreamPhysicalSink.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE](input=HepRelVertex#208,table=default_catalog.default_database.Unregistered_Collect_Sink_1,fields=product_id, max_price, avg_price)
2023-03-07 15:58:56  [ main:1570 ] - [ DEBUG ]  
For final plan, using rel#207:StreamPhysicalCalc.STREAM_PHYSICAL.any.None: 0.[I].[NONE](input=HepRelVertex#206,select=product_id, w0$o0 AS max_price, /(CASE(>(w0$o1, 0:BIGINT), w0$o2, null:DOUBLE), w0$o1) AS avg_price)
2023-03-07 15:58:56  [ main:1570 ] - [ DEBUG ]  
For final plan, using rel#205:StreamPhysicalOverAggregate.STREAM_PHYSICAL.any.None: 0.[I].[NONE](input=HepRelVertex#204,partitionBy=product_id,orderBy=date_time ASC,window= ROWS BETWEEN 3 PRECEDING AND CURRENT ROW,select=product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2)
2023-03-07 15:58:56  [ main:1571 ] - [ DEBUG ]  
For final plan, using rel#203:StreamPhysicalExchange.STREAM_PHYSICAL.hash[0]true.None: 0.[I].[NONE](input=HepRelVertex#202,distribution=hash[product_id])
2023-03-07 15:58:56  [ main:1571 ] - [ DEBUG ]  
For final plan, using rel#201:StreamPhysicalCalc.STREAM_PHYSICAL.any.None: 0.[I].[NONE](input=HepRelVertex#200,select=product_id, price, date_time)
2023-03-07 15:58:56  [ main:1571 ] - [ DEBUG ]  
For final plan, using rel#182:StreamPhysicalDataStreamScan.STREAM_PHYSICAL.any.None: 0.[I].[NONE](table=[default_catalog, default_database, UnnamedTable$0],fields=product_id, buyer_name, price, date_time)
2023-03-07 15:58:56  [ main:1572 ] - [ DEBUG ]  
optimize physical rewrite cost 2 ms.
optimize result:
 Sink(table=[default_catalog.default_database.Unregistered_Collect_Sink_1], fields=[product_id, max_price, avg_price])
+- Calc(select=[product_id, w0$o0 AS max_price, /(CASE(>(w0$o1, 0:BIGINT), w0$o2, null:DOUBLE), w0$o1) AS avg_price])
   +- OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2])
      +- Exchange(distribution=[hash[product_id]])
         +- Calc(select=[product_id, price, date_time])
            +- DataStreamScan(table=[[default_catalog, default_database, UnnamedTable$0]], fields=[product_id, buyer_name, price, date_time])

2023-03-07 15:58:56  [ main:1572 ] - [ DEBUG ]  
optimize physical_rewrite cost 21 ms.
optimize result: 
Sink(table=[default_catalog.default_database.Unregistered_Collect_Sink_1], fields=[product_id, max_price, avg_price])
+- Calc(select=[product_id, w0$o0 AS max_price, /(CASE(>(w0$o1, 0:BIGINT), w0$o2, null:DOUBLE), w0$o1) AS avg_price])
   +- OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2])
      +- Exchange(distribution=[hash[product_id]])
         +- Calc(select=[product_id, price, date_time])
            +- DataStreamScan(table=[[default_catalog, default_database, UnnamedTable$0]], fields=[product_id, buyer_name, price, date_time])

2023-03-07 15:58:56  [ main:1696 ] - [ DEBUG ]  
Compiling OneInputStreamOperator Code:
SourceConversion
2023-03-07 15:58:56  [ main:1844 ] - [ DEBUG ]  
Compiling OneInputStreamOperator Code:
StreamExecCalc
2023-03-07 15:58:56  [ main:2002 ] - [ DEBUG ]  
Got FunctionDefinition 'isnull' from 'core' module.
2023-03-07 15:58:56  [ main:2012 ] - [ DEBUG ]  
Got FunctionDefinition 'plus' from 'core' module.
2023-03-07 15:58:56  [ main:2015 ] - [ DEBUG ]  
Got FunctionDefinition 'ifthenelse' from 'core' module.
2023-03-07 15:58:57  [ main:2023 ] - [ DEBUG ]  
Got FunctionDefinition 'isnull' from 'core' module.
2023-03-07 15:58:57  [ main:2023 ] - [ DEBUG ]  
Got FunctionDefinition 'plus' from 'core' module.
2023-03-07 15:58:57  [ main:2023 ] - [ DEBUG ]  
Got FunctionDefinition 'ifthenelse' from 'core' module.
2023-03-07 15:58:57  [ main:2024 ] - [ DEBUG ]  
Got FunctionDefinition 'cast' from 'core' module.
2023-03-07 15:58:57  [ main:2025 ] - [ DEBUG ]  
Got FunctionDefinition 'plus' from 'core' module.
2023-03-07 15:58:57  [ main:2027 ] - [ DEBUG ]  
Got FunctionDefinition 'isnull' from 'core' module.
2023-03-07 15:58:57  [ main:2027 ] - [ DEBUG ]  
Got FunctionDefinition 'minus' from 'core' module.
2023-03-07 15:58:57  [ main:2027 ] - [ DEBUG ]  
Got FunctionDefinition 'ifthenelse' from 'core' module.
2023-03-07 15:58:57  [ main:2029 ] - [ DEBUG ]  
Got FunctionDefinition 'isnull' from 'core' module.
2023-03-07 15:58:57  [ main:2030 ] - [ DEBUG ]  
Got FunctionDefinition 'minus' from 'core' module.
2023-03-07 15:58:57  [ main:2030 ] - [ DEBUG ]  
Got FunctionDefinition 'ifthenelse' from 'core' module.
2023-03-07 15:58:57  [ main:2030 ] - [ DEBUG ]  
Got FunctionDefinition 'cast' from 'core' module.
2023-03-07 15:58:57  [ main:2031 ] - [ DEBUG ]  
Got FunctionDefinition 'minus' from 'core' module.
2023-03-07 15:58:57  [ main:2060 ] - [ DEBUG ]  
Compiling OneInputStreamOperator Code:
StreamExecCalc
2023-03-07 15:58:57  [ main:2090 ] - [ DEBUG ]  
Transforming LegacySinkTransformation{id=10, name='Collect table sink', outputType=GenericType<java.lang.Object>, parallelism=1}
2023-03-07 15:58:57  [ main:2091 ] - [ DEBUG ]  
Transforming OneInputTransformation{id=8, name='Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price])', outputType=ROW<`product_id` INT, `max_price` DOUBLE, `avg_price` DOUBLE>(org.apache.flink.table.data.RowData, org.apache.flink.table.runtime.typeutils.RowDataSerializer), parallelism=-1}
2023-03-07 15:58:57  [ main:2092 ] - [ DEBUG ]  
Transforming OneInputTransformation{id=7, name='OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2])', outputType=ROW<`product_id` INT, `price` DOUBLE, `date_time` TIMESTAMP(3) *ROWTIME*, `w0$o0` DOUBLE, `w0$o1` BIGINT NOT NULL, `w0$o2` DOUBLE NOT NULL>(org.apache.flink.table.data.RowData, org.apache.flink.table.runtime.typeutils.RowDataSerializer), parallelism=-1}
2023-03-07 15:58:57  [ main:2092 ] - [ DEBUG ]  
Transforming PartitionTransformation{id=6, name='Partition', outputType=ROW<`product_id` INT, `price` DOUBLE, `date_time` TIMESTAMP(3) *ROWTIME*>(org.apache.flink.table.data.RowData, org.apache.flink.table.runtime.typeutils.RowDataSerializer), parallelism=-1}
2023-03-07 15:58:57  [ main:2092 ] - [ DEBUG ]  
Transforming OneInputTransformation{id=5, name='Calc(select=[product_id, price, date_time])', outputType=ROW<`product_id` INT, `price` DOUBLE, `date_time` TIMESTAMP(3) *ROWTIME*>(org.apache.flink.table.data.RowData, org.apache.flink.table.runtime.typeutils.RowDataSerializer), parallelism=1}
2023-03-07 15:58:57  [ main:2093 ] - [ DEBUG ]  
Transforming OneInputTransformation{id=4, name='SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time])', outputType=ROW<`product_id` INT, `buyer_name` STRING, `price` DOUBLE, `date_time` TIMESTAMP(3) *ROWTIME*>(org.apache.flink.table.data.RowData, org.apache.flink.table.runtime.typeutils.RowDataSerializer), parallelism=1}
2023-03-07 15:58:57  [ main:2096 ] - [ DEBUG ]  
Transforming TimestampsAndWatermarksTransformation{id=3, name='Timestamps/Watermarks', outputType=PojoType<org.fllik.geektime.flinksql.Userproduct, fields = [buyer_name: String, date_time: Long, price: Double, product_id: Integer]>, parallelism=1}
2023-03-07 15:58:57  [ main:2096 ] - [ DEBUG ]  
Transforming OneInputTransformation{id=2, name='Map', outputType=PojoType<org.fllik.geektime.flinksql.Userproduct, fields = [buyer_name: String, date_time: Long, price: Double, product_id: Integer]>, parallelism=1}
2023-03-07 15:58:57  [ main:2096 ] - [ DEBUG ]  
Transforming LegacySourceTransformation{id=1, name='Socket Stream', outputType=String, parallelism=1}
2023-03-07 15:58:57  [ main:2099 ] - [ DEBUG ]  
Vertex: 1
2023-03-07 15:58:57  [ main:2107 ] - [ DEBUG ]  
Vertex: 2
2023-03-07 15:58:57  [ main:2109 ] - [ DEBUG ]  
Vertex: 3
2023-03-07 15:58:57  [ main:2109 ] - [ DEBUG ]  
Vertex: 4
2023-03-07 15:58:57  [ main:2109 ] - [ DEBUG ]  
Vertex: 5
2023-03-07 15:58:57  [ main:2115 ] - [ DEBUG ]  
Vertex: 7
2023-03-07 15:58:57  [ main:2115 ] - [ DEBUG ]  
Vertex: 8
2023-03-07 15:58:57  [ main:2116 ] - [ DEBUG ]  
Vertex: 10
2023-03-07 15:58:57  [ main:2143 ] - [ DEBUG ]  
Generated hash 'cbc357ccb763df2852fee8c4fc7d55f2' for node 'Source: Socket Stream-1' {id: 1, parallelism: 1, user function: org.apache.flink.streaming.api.functions.source.SocketTextStreamFunction}
2023-03-07 15:58:57  [ main:2143 ] - [ DEBUG ]  
Generated hash '570f707193e0fe32f4d86d067aba243b' for node 'Map-2' {id: 2, parallelism: 1, user function: org.fllik.geektime.flinksql.FlinkSQLOverWinEvRowRange$$Lambda$233/1647243882}
2023-03-07 15:58:57  [ main:2143 ] - [ DEBUG ]  
Generated hash 'ba40499bacce995f15693b1735928377' for node 'Timestamps/Watermarks-3' {id: 3, parallelism: 1, user function: }
2023-03-07 15:58:57  [ main:2143 ] - [ DEBUG ]  
Generated hash 'cf155f65686cb012844f7c745ec70a3c' for node 'SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time])-4' {id: 4, parallelism: 1, user function: }
2023-03-07 15:58:57  [ main:2144 ] - [ DEBUG ]  
Generated hash '7f86b06891c19f1e76c3f65c90ce752b' for node 'Calc(select=[product_id, price, date_time])-5' {id: 5, parallelism: 1, user function: }
2023-03-07 15:58:57  [ main:2144 ] - [ DEBUG ]  
Generated hash '337adade1e207453ed3502e01d75fd03' for node 'OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2])-7' {id: 7, parallelism: 1, user function: org.apache.flink.table.runtime.operators.over.RowTimeRowsBoundedPrecedingFunction}
2023-03-07 15:58:57  [ main:2144 ] - [ DEBUG ]  
Generated hash 'f2ba30a9177488df4b4ed4a5e3ce5903' for node 'Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price])-8' {id: 8, parallelism: 1, user function: }
2023-03-07 15:58:57  [ main:2144 ] - [ DEBUG ]  
Generated hash 'b58b2e9f59592b8492696c6eaf245059' for node 'Sink: Collect table sink-10' {id: 10, parallelism: 1, user function: org.apache.flink.streaming.api.operators.collect.CollectSinkFunction}
2023-03-07 15:58:57  [ main:2179 ] - [ DEBUG ]  
Parallelism set: 1 for 7
2023-03-07 15:58:57  [ main:2193 ] - [ DEBUG ]  
Parallelism set: 1 for 1
2023-03-07 15:58:57  [ main:2197 ] - [ DEBUG ]  
CONNECTED: KeyGroupStreamPartitioner - 1 -> 7
2023-03-07 15:58:57  [ main:2223 ] - [ INFO ]  
The configuration option taskmanager.cpu.cores required for local execution is not set, setting it to the maximal possible value.
2023-03-07 15:58:57  [ main:2223 ] - [ INFO ]  
The configuration option taskmanager.memory.task.heap.size required for local execution is not set, setting it to the maximal possible value.
2023-03-07 15:58:57  [ main:2223 ] - [ INFO ]  
The configuration option taskmanager.memory.task.off-heap.size required for local execution is not set, setting it to the maximal possible value.
2023-03-07 15:58:57  [ main:2224 ] - [ INFO ]  
The configuration option taskmanager.memory.network.min required for local execution is not set, setting it to its default value 64 mb.
2023-03-07 15:58:57  [ main:2224 ] - [ INFO ]  
The configuration option taskmanager.memory.network.max required for local execution is not set, setting it to its default value 64 mb.
2023-03-07 15:58:57  [ main:2224 ] - [ INFO ]  
The configuration option taskmanager.memory.managed.size required for local execution is not set, setting it to its default value 128 mb.
2023-03-07 15:58:57  [ main:2226 ] - [ INFO ]  
Starting Flink Mini Cluster
2023-03-07 15:58:57  [ main:2226 ] - [ DEBUG ]  
Using configuration MiniClusterConfiguration {singleRpcService=SHARED, numTaskManagers=1, commonBindAddress='null', config={taskmanager.memory.network.min=64 mb, taskmanager.cpu.cores=1000000.0, taskmanager.memory.task.off-heap.size=1099511627776 bytes, taskmanager.memory.jvm-metaspace.size=256 mb, execution.target=local, cluster.io-pool.size=4, execution.runtime-mode=STREAMING, taskmanager.memory.jvm-overhead.min=1 gb, rest.bind-port=0, taskmanager.memory.network.max=64 mb, taskmanager.memory.framework.off-heap.size=128 mb, execution.attached=true, taskmanager.memory.managed.size=128 mb, taskmanager.memory.framework.heap.size=128 mb, parallelism.default=16, taskmanager.numberOfTaskSlots=1, taskmanager.memory.task.heap.size=1099511627776 bytes, rest.address=localhost, taskmanager.memory.jvm-overhead.max=1 gb, akka.ask.timeout=PT5M}}
2023-03-07 15:58:58  [ main:3155 ] - [ INFO ]  
Starting Metrics Registry
2023-03-07 15:58:58  [ main:3188 ] - [ INFO ]  
No metrics reporter configured, no metrics will be exposed/reported.
2023-03-07 15:58:58  [ main:3189 ] - [ INFO ]  
Starting RPC Service(s)
2023-03-07 15:58:58  [ main:3198 ] - [ INFO ]  
Trying to start local actor system
2023-03-07 15:58:58  [ main:3202 ] - [ DEBUG ]  
Using akka configuration
 Config(SimpleConfigObject({"akka":{"actor":{"allow-java-serialization":"on","default-dispatcher":{"executor":"fork-join-executor","fork-join-executor":{"parallelism-factor":1,"parallelism-max":4,"parallelism-min":2},"throughput":15},"guardian-supervisor-strategy":"org.apache.flink.runtime.rpc.akka.EscalatingSupervisorStrategy","supervisor-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"type":"Dispatcher"},"warn-about-java-serializer-usage":"off"},"daemonic":"off","jvm-exit-on-fatal-error":"on","log-config-on-start":"off","log-dead-letters":"off","log-dead-letters-during-shutdown":"off","logger-startup-timeout":"50s","loggers":["akka.event.slf4j.Slf4jLogger"],"logging-filter":"akka.event.slf4j.Slf4jLoggingFilter","loglevel":"DEBUG","serialize-messages":"off","stdout-loglevel":"OFF"}}))
2023-03-07 15:58:58  [ flink-akka.actor.default-dispatcher-4:3838 ] - [ INFO ]  
Slf4jLogger started
2023-03-07 15:58:58  [ flink-akka.actor.default-dispatcher-4:3848 ] - [ DEBUG ]  
logger log1-Slf4jLogger started
2023-03-07 15:58:58  [ flink-akka.actor.default-dispatcher-4:3848 ] - [ DEBUG ]  
Default Loggers started
2023-03-07 15:58:58  [ main:3939 ] - [ INFO ]  
Actor system started at akka://flink
2023-03-07 15:58:58  [ main:3951 ] - [ INFO ]  
Trying to start local actor system
2023-03-07 15:58:58  [ main:3951 ] - [ DEBUG ]  
Using akka configuration
 Config(SimpleConfigObject({"akka":{"actor":{"allow-java-serialization":"on","default-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"thread-priority":1,"throughput":15,"type":"org.apache.flink.runtime.rpc.akka.PriorityThreadsDispatcher"},"guardian-supervisor-strategy":"org.apache.flink.runtime.rpc.akka.EscalatingSupervisorStrategy","supervisor-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"type":"Dispatcher"},"warn-about-java-serializer-usage":"off"},"daemonic":"off","jvm-exit-on-fatal-error":"on","log-config-on-start":"off","log-dead-letters":"off","log-dead-letters-during-shutdown":"off","logger-startup-timeout":"50s","loggers":["akka.event.slf4j.Slf4jLogger"],"logging-filter":"akka.event.slf4j.Slf4jLoggingFilter","loglevel":"DEBUG","serialize-messages":"off","stdout-loglevel":"OFF"}}))
2023-03-07 15:58:58  [ flink-metrics-5:3960 ] - [ INFO ]  
Slf4jLogger started
2023-03-07 15:58:58  [ flink-metrics-5:3961 ] - [ DEBUG ]  
logger log1-Slf4jLogger started
2023-03-07 15:58:58  [ flink-metrics-5:3962 ] - [ DEBUG ]  
Default Loggers started
2023-03-07 15:58:58  [ main:3968 ] - [ INFO ]  
Actor system started at akka://flink-metrics
2023-03-07 15:58:58  [ flink-metrics-akka.actor.supervisor-dispatcher-6:3977 ] - [ DEBUG ]  
Starting AkkaRpcActor with name MetricQueryService.
2023-03-07 15:58:58  [ main:3979 ] - [ INFO ]  
Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .
2023-03-07 15:58:59  [ main:4024 ] - [ INFO ]  
Starting high-availability services
2023-03-07 15:58:59  [ main:4033 ] - [ INFO ]  
Created BLOB server storage directory C:\Users\ADMINI~1\AppData\Local\Temp\blobStore-30c46615-a0e7-4083-94f4-631b5fd00cbb
2023-03-07 15:58:59  [ main:4037 ] - [ DEBUG ]  
Trying to open socket on port 0
2023-03-07 15:58:59  [ main:4037 ] - [ INFO ]  
Started BLOB server at 0.0.0.0:63048 - max concurrent requests: 50 - max backlog: 1000
2023-03-07 15:58:59  [ main:4048 ] - [ INFO ]  
Created BLOB cache storage directory C:\Users\ADMINI~1\AppData\Local\Temp\blobStore-b2cdfc65-7ced-4236-9acb-f638fed269ad
2023-03-07 15:58:59  [ main:4050 ] - [ INFO ]  
Created BLOB cache storage directory C:\Users\ADMINI~1\AppData\Local\Temp\blobStore-448c3321-345a-423c-b6b6-e5892f3f862f
2023-03-07 15:58:59  [ main:4050 ] - [ INFO ]  
Starting 1 TaskManger(s)
2023-03-07 15:58:59  [ main:4054 ] - [ INFO ]  
Starting TaskManager with ResourceID: e6aa20be-363d-4d66-b022-cceae3fbad81
2023-03-07 15:58:59  [ main:4065 ] - [ INFO ]  
Temporary file directory 'C:\Users\ADMINI~1\AppData\Local\Temp': total 476 GB, usable 281 GB (59.03% usable)
2023-03-07 15:58:59  [ main:4068 ] - [ DEBUG ]  
FileChannelManager uses directory C:\Users\ADMINI~1\AppData\Local\Temp\flink-io-442b97b6-8405-49fb-88a6-7995a0618652 for spill files.
2023-03-07 15:58:59  [ main:4068 ] - [ INFO ]  
Created a new FileChannelManager for spilling of task related data to disk (joins, sorting, ...). Used directories:
	C:\Users\ADMINI~1\AppData\Local\Temp\flink-io-442b97b6-8405-49fb-88a6-7995a0618652
2023-03-07 15:58:59  [ main:4076 ] - [ DEBUG ]  
FileChannelManager uses directory C:\Users\ADMINI~1\AppData\Local\Temp\flink-netty-shuffle-aeafdd9c-a480-4aa7-9fe2-60b96ba150df for spill files.
2023-03-07 15:58:59  [ main:4076 ] - [ INFO ]  
Created a new FileChannelManager for storing result partitions of BLOCKING shuffles. Used directories:
	C:\Users\ADMINI~1\AppData\Local\Temp\flink-netty-shuffle-aeafdd9c-a480-4aa7-9fe2-60b96ba150df
2023-03-07 15:58:59  [ main:4110 ] - [ INFO ]  
Allocated 64 MB for network buffer pool (number of memory segments: 2048, bytes per segment: 32768).
2023-03-07 15:58:59  [ main:4118 ] - [ INFO ]  
Starting the network environment and its components.
2023-03-07 15:58:59  [ main:4118 ] - [ DEBUG ]  
Starting network connection manager
2023-03-07 15:58:59  [ main:4120 ] - [ INFO ]  
Starting the kvState service and its components.
2023-03-07 15:58:59  [ main:4130 ] - [ DEBUG ]  
Messages have a max timeout of 300000 ms
2023-03-07 15:58:59  [ main:4130 ] - [ INFO ]  
Config uses fallback configuration key 'akka.ask.timeout' instead of key 'taskmanager.slot.timeout'
2023-03-07 15:58:59  [ flink-akka.actor.supervisor-dispatcher-5:4140 ] - [ DEBUG ]  
Starting AkkaRpcActor with name taskmanager_0.
2023-03-07 15:58:59  [ main:4140 ] - [ INFO ]  
Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
2023-03-07 15:58:59  [ flink-akka.actor.default-dispatcher-4:4152 ] - [ INFO ]  
Start job leader service.
2023-03-07 15:58:59  [ flink-akka.actor.default-dispatcher-4:4153 ] - [ INFO ]  
User file cache uses directory C:\Users\ADMINI~1\AppData\Local\Temp\flink-dist-cache-6fbf0f17-c0cc-4709-a36e-c9f7ff453feb
2023-03-07 15:58:59  [ main:4190 ] - [ DEBUG ]  
Starting Dispatcher REST endpoint.
2023-03-07 15:58:59  [ main:4190 ] - [ INFO ]  
Starting rest endpoint.
2023-03-07 15:58:59  [ main:4192 ] - [ DEBUG ]  
Failed to load web based job submission extension.
org.apache.flink.util.FlinkException: The module flink-runtime-web could not be found in the class path. Please add this jar in order to enable web based job submission.
	at org.apache.flink.runtime.webmonitor.WebMonitorUtils.loadWebSubmissionExtension(WebMonitorUtils.java:197)
	at org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint.initializeWebSubmissionHandlers(DispatcherRestEndpoint.java:110)
	at org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.initializeHandlers(WebMonitorEndpoint.java:262)
	at org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint.initializeHandlers(DispatcherRestEndpoint.java:87)
	at org.apache.flink.runtime.rest.RestServerEndpoint.start(RestServerEndpoint.java:178)
	at org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory.create(DefaultDispatcherResourceManagerComponentFactory.java:172)
	at org.apache.flink.runtime.minicluster.MiniCluster.createDispatcherResourceManagerComponents(MiniCluster.java:477)
	at org.apache.flink.runtime.minicluster.MiniCluster.setupDispatcherResourceManagerComponents(MiniCluster.java:436)
	at org.apache.flink.runtime.minicluster.MiniCluster.start(MiniCluster.java:380)
	at org.apache.flink.client.program.PerJobMiniClusterFactory.submitJob(PerJobMiniClusterFactory.java:75)
	at org.apache.flink.client.deployment.executors.LocalExecutor.execute(LocalExecutor.java:85)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:2043)
	at org.apache.flink.table.planner.delegation.DefaultExecutor.executeAsync(DefaultExecutor.java:95)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:811)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1274)
	at org.apache.flink.table.api.internal.TableImpl.execute(TableImpl.java:601)
	at org.fllik.geektime.flinksql.FlinkSQLOverWinEvRowRange.main(FlinkSQLOverWinEvRowRange.java:58)
2023-03-07 15:58:59  [ main:4321 ] - [ DEBUG ]  
Using SLF4J as the default logging framework
2023-03-07 15:58:59  [ main:4321 ] - [ DEBUG ]  
-Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
2023-03-07 15:58:59  [ main:4321 ] - [ DEBUG ]  
-Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
2023-03-07 15:58:59  [ main:4380 ] - [ WARN ]  
Log file environment variable 'log.file' is not set.
2023-03-07 15:58:59  [ main:4380 ] - [ WARN ]  
JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable 'log.file' or configuration key 'web.log.path'.
2023-03-07 15:58:59  [ main:4410 ] - [ DEBUG ]  
-Dio.netty.noUnsafe: false
2023-03-07 15:58:59  [ main:4411 ] - [ DEBUG ]  
Java version: 8
2023-03-07 15:58:59  [ main:4411 ] - [ DEBUG ]  
sun.misc.Unsafe.theUnsafe: available
2023-03-07 15:58:59  [ main:4412 ] - [ DEBUG ]  
sun.misc.Unsafe.copyMemory: available
2023-03-07 15:58:59  [ main:4412 ] - [ DEBUG ]  
java.nio.Buffer.address: available
2023-03-07 15:58:59  [ main:4413 ] - [ DEBUG ]  
direct buffer constructor: available
2023-03-07 15:58:59  [ main:4414 ] - [ DEBUG ]  
java.nio.Bits.unaligned: available, true
2023-03-07 15:58:59  [ main:4414 ] - [ DEBUG ]  
jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
2023-03-07 15:58:59  [ main:4414 ] - [ DEBUG ]  
java.nio.DirectByteBuffer.<init>(long, int): available
2023-03-07 15:58:59  [ main:4414 ] - [ DEBUG ]  
sun.misc.Unsafe: available
2023-03-07 15:58:59  [ main:4414 ] - [ DEBUG ]  
-Dio.netty.tmpdir: C:\Users\ADMINI~1\AppData\Local\Temp (java.io.tmpdir)
2023-03-07 15:58:59  [ main:4414 ] - [ DEBUG ]  
-Dio.netty.bitMode: 64 (sun.arch.data.model)
2023-03-07 15:58:59  [ main:4414 ] - [ DEBUG ]  
Platform: Windows
2023-03-07 15:58:59  [ main:4415 ] - [ DEBUG ]  
-Dio.netty.maxDirectMemory: 7497842688 bytes
2023-03-07 15:58:59  [ main:4415 ] - [ DEBUG ]  
-Dio.netty.uninitializedArrayAllocationThreshold: -1
2023-03-07 15:58:59  [ main:4416 ] - [ DEBUG ]  
java.nio.ByteBuffer.cleaner(): available
2023-03-07 15:58:59  [ main:4416 ] - [ DEBUG ]  
-Dio.netty.noPreferDirect: false
2023-03-07 15:58:59  [ main:4419 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.cluster.ShutdownHandler@55c8fc60 under DELETE@/v1/cluster.
2023-03-07 15:58:59  [ main:4420 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.cluster.ShutdownHandler@55c8fc60 under DELETE@/cluster.
2023-03-07 15:58:59  [ main:4420 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.cluster.DashboardConfigHandler@22172b00 under GET@/v1/config.
2023-03-07 15:58:59  [ main:4420 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.cluster.DashboardConfigHandler@22172b00 under GET@/config.
2023-03-07 15:58:59  [ main:4420 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetListHandler@d02c00 under GET@/v1/datasets.
2023-03-07 15:58:59  [ main:4420 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetListHandler@d02c00 under GET@/datasets.
2023-03-07 15:58:59  [ main:4420 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteStatusHandler@4205d5d0 under GET@/v1/datasets/delete/:triggerid.
2023-03-07 15:58:59  [ main:4420 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteStatusHandler@4205d5d0 under GET@/datasets/delete/:triggerid.
2023-03-07 15:58:59  [ main:4420 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteTriggerHandler@29dfc68f under DELETE@/v1/datasets/:datasetid.
2023-03-07 15:58:59  [ main:4420 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteTriggerHandler@29dfc68f under DELETE@/datasets/:datasetid.
2023-03-07 15:58:59  [ main:4420 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterConfigHandler@320ba79f under GET@/v1/jobmanager/config.
2023-03-07 15:58:59  [ main:4420 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterConfigHandler@320ba79f under GET@/jobmanager/config.
2023-03-07 15:58:59  [ main:4420 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@333a2df2 under GET@/v1/jobmanager/log.
2023-03-07 15:58:59  [ main:4420 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@333a2df2 under GET@/jobmanager/log.
2023-03-07 15:58:59  [ main:4420 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandler@5ffdd510 under GET@/v1/jobmanager/logs.
2023-03-07 15:58:59  [ main:4420 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandler@5ffdd510 under GET@/jobmanager/logs.
2023-03-07 15:58:59  [ main:4420 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerCustomLogHandler@8c18bde under GET@/v1/jobmanager/logs/:filename.
2023-03-07 15:58:59  [ main:4420 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerCustomLogHandler@8c18bde under GET@/jobmanager/logs/:filename.
2023-03-07 15:58:59  [ main:4420 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobManagerMetricsHandler@6719f206 under GET@/v1/jobmanager/metrics.
2023-03-07 15:58:59  [ main:4420 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobManagerMetricsHandler@6719f206 under GET@/jobmanager/metrics.
2023-03-07 15:58:59  [ main:4420 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@5ae16aa under GET@/v1/jobmanager/stdout.
2023-03-07 15:58:59  [ main:4420 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@5ae16aa under GET@/jobmanager/stdout.
2023-03-07 15:58:59  [ main:4421 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.JobIdsHandler@344a065a under GET@/v1/jobs.
2023-03-07 15:58:59  [ main:4421 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.JobIdsHandler@344a065a under GET@/jobs.
2023-03-07 15:58:59  [ main:4421 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.JobSubmitHandler@3051e476 under POST@/v1/jobs.
2023-03-07 15:58:59  [ main:4421 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.JobSubmitHandler@3051e476 under POST@/jobs.
2023-03-07 15:58:59  [ main:4421 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingJobsMetricsHandler@1870b9b8 under GET@/v1/jobs/metrics.
2023-03-07 15:58:59  [ main:4421 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingJobsMetricsHandler@1870b9b8 under GET@/jobs/metrics.
2023-03-07 15:58:59  [ main:4421 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler@6d9ee75a under GET@/v1/jobs/overview.
2023-03-07 15:58:59  [ main:4421 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler@6d9ee75a under GET@/jobs/overview.
2023-03-07 15:58:59  [ main:4421 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.JobDetailsHandler@9825465 under GET@/v1/jobs/:jobid.
2023-03-07 15:58:59  [ main:4421 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.JobDetailsHandler@9825465 under GET@/jobs/:jobid.
2023-03-07 15:58:59  [ main:4421 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@36cf6377 under PATCH@/v1/jobs/:jobid.
2023-03-07 15:58:59  [ main:4421 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@36cf6377 under PATCH@/jobs/:jobid.
2023-03-07 15:58:59  [ main:4421 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.JobAccumulatorsHandler@2befb16f under GET@/v1/jobs/:jobid/accumulators.
2023-03-07 15:58:59  [ main:4421 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.JobAccumulatorsHandler@2befb16f under GET@/jobs/:jobid/accumulators.
2023-03-07 15:58:59  [ main:4421 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler@3151277f under GET@/v1/jobs/:jobid/checkpoints.
2023-03-07 15:58:59  [ main:4421 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler@3151277f under GET@/jobs/:jobid/checkpoints.
2023-03-07 15:58:59  [ main:4421 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler@2aeb7c4c under GET@/v1/jobs/:jobid/checkpoints/config.
2023-03-07 15:58:59  [ main:4421 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler@2aeb7c4c under GET@/jobs/:jobid/checkpoints/config.
2023-03-07 15:58:59  [ main:4421 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointStatisticDetailsHandler@67f266bd under GET@/v1/jobs/:jobid/checkpoints/details/:checkpointid.
2023-03-07 15:58:59  [ main:4421 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointStatisticDetailsHandler@67f266bd under GET@/jobs/:jobid/checkpoints/details/:checkpointid.
2023-03-07 15:58:59  [ main:4421 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.TaskCheckpointStatisticDetailsHandler@31c0c7e5 under GET@/v1/jobs/:jobid/checkpoints/details/:checkpointid/subtasks/:vertexid.
2023-03-07 15:58:59  [ main:4421 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.TaskCheckpointStatisticDetailsHandler@31c0c7e5 under GET@/jobs/:jobid/checkpoints/details/:checkpointid/subtasks/:vertexid.
2023-03-07 15:58:59  [ main:4421 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.JobConfigHandler@912747d under GET@/v1/jobs/:jobid/config.
2023-03-07 15:58:59  [ main:4421 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.JobConfigHandler@912747d under GET@/jobs/:jobid/config.
2023-03-07 15:58:59  [ main:4421 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.coordination.ClientCoordinationHandler@78ea700f under POST@/v1/jobs/:jobid/coordinators/:operatorid.
2023-03-07 15:58:59  [ main:4421 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.coordination.ClientCoordinationHandler@78ea700f under POST@/jobs/:jobid/coordinators/:operatorid.
2023-03-07 15:58:59  [ main:4421 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler@cd93621 under GET@/v1/jobs/:jobid/exceptions.
2023-03-07 15:58:59  [ main:4421 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler@cd93621 under GET@/jobs/:jobid/exceptions.
2023-03-07 15:58:59  [ main:4421 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler@3b1137b0 under GET@/v1/jobs/:jobid/execution-result.
2023-03-07 15:58:59  [ main:4421 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler@3b1137b0 under GET@/jobs/:jobid/execution-result.
2023-03-07 15:58:59  [ main:4421 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobMetricsHandler@21ba0d33 under GET@/v1/jobs/:jobid/metrics.
2023-03-07 15:58:59  [ main:4421 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobMetricsHandler@21ba0d33 under GET@/jobs/:jobid/metrics.
2023-03-07 15:58:59  [ main:4421 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.JobPlanHandler@afb7b03 under GET@/v1/jobs/:jobid/plan.
2023-03-07 15:58:59  [ main:4421 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.JobPlanHandler@afb7b03 under GET@/jobs/:jobid/plan.
2023-03-07 15:58:59  [ main:4421 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingTriggerHandler@4fd63c43 under PATCH@/v1/jobs/:jobid/rescaling.
2023-03-07 15:58:59  [ main:4421 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingTriggerHandler@4fd63c43 under PATCH@/jobs/:jobid/rescaling.
2023-03-07 15:58:59  [ main:4421 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingStatusHandler@7d483ebe under GET@/v1/jobs/:jobid/rescaling/:triggerid.
2023-03-07 15:58:59  [ main:4421 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingStatusHandler@7d483ebe under GET@/jobs/:jobid/rescaling/:triggerid.
2023-03-07 15:58:59  [ main:4422 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointTriggerHandler@cea67b1 under POST@/v1/jobs/:jobid/savepoints.
2023-03-07 15:58:59  [ main:4422 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointTriggerHandler@cea67b1 under POST@/jobs/:jobid/savepoints.
2023-03-07 15:58:59  [ main:4422 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointStatusHandler@5d98364c under GET@/v1/jobs/:jobid/savepoints/:triggerid.
2023-03-07 15:58:59  [ main:4422 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointStatusHandler@5d98364c under GET@/jobs/:jobid/savepoints/:triggerid.
2023-03-07 15:58:59  [ main:4422 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$StopWithSavepointHandler@23d23d98 under POST@/v1/jobs/:jobid/stop.
2023-03-07 15:58:59  [ main:4422 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$StopWithSavepointHandler@23d23d98 under POST@/jobs/:jobid/stop.
2023-03-07 15:58:59  [ main:4422 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler@1fd35a92 under GET@/v1/jobs/:jobid/vertices/:vertexid.
2023-03-07 15:58:59  [ main:4422 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler@1fd35a92 under GET@/jobs/:jobid/vertices/:vertexid.
2023-03-07 15:58:59  [ main:4422 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.JobVertexAccumulatorsHandler@40db6136 under GET@/v1/jobs/:jobid/vertices/:vertexid/accumulators.
2023-03-07 15:58:59  [ main:4422 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.JobVertexAccumulatorsHandler@40db6136 under GET@/jobs/:jobid/vertices/:vertexid/accumulators.
2023-03-07 15:58:59  [ main:4422 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandler@5ced0537 under GET@/v1/jobs/:jobid/vertices/:vertexid/backpressure.
2023-03-07 15:58:59  [ main:4422 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandler@5ced0537 under GET@/jobs/:jobid/vertices/:vertexid/backpressure.
2023-03-07 15:58:59  [ main:4422 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandler$DisabledJobVertexFlameGraphHandler@6ee1ddcf under GET@/v1/jobs/:jobid/vertices/:vertexid/flamegraph.
2023-03-07 15:58:59  [ main:4422 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandler$DisabledJobVertexFlameGraphHandler@6ee1ddcf under GET@/jobs/:jobid/vertices/:vertexid/flamegraph.
2023-03-07 15:58:59  [ main:4422 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler@6b630d4b under GET@/v1/jobs/:jobid/vertices/:vertexid/metrics.
2023-03-07 15:58:59  [ main:4422 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler@6b630d4b under GET@/jobs/:jobid/vertices/:vertexid/metrics.
2023-03-07 15:58:59  [ main:4422 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.SubtasksAllAccumulatorsHandler@70aa03c0 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/accumulators.
2023-03-07 15:58:59  [ main:4422 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.SubtasksAllAccumulatorsHandler@70aa03c0 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/accumulators.
2023-03-07 15:58:59  [ main:4422 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingSubtasksMetricsHandler@44da745f under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/metrics.
2023-03-07 15:58:59  [ main:4422 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingSubtasksMetricsHandler@44da745f under GET@/jobs/:jobid/vertices/:vertexid/subtasks/metrics.
2023-03-07 15:58:59  [ main:4422 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandler@2435c6ae under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex.
2023-03-07 15:58:59  [ main:4422 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandler@2435c6ae under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex.
2023-03-07 15:58:59  [ main:4422 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandler@574f9e36 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt.
2023-03-07 15:58:59  [ main:4422 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandler@574f9e36 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt.
2023-03-07 15:58:59  [ main:4422 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandler@77db231c under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt/accumulators.
2023-03-07 15:58:59  [ main:4422 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandler@77db231c under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt/accumulators.
2023-03-07 15:58:59  [ main:4422 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.metrics.SubtaskMetricsHandler@60b4d934 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/metrics.
2023-03-07 15:58:59  [ main:4422 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.metrics.SubtaskMetricsHandler@60b4d934 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/metrics.
2023-03-07 15:58:59  [ main:4422 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.SubtasksTimesHandler@6a3e633a under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasktimes.
2023-03-07 15:58:59  [ main:4422 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.SubtasksTimesHandler@6a3e633a under GET@/jobs/:jobid/vertices/:vertexid/subtasktimes.
2023-03-07 15:58:59  [ main:4422 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.JobVertexTaskManagersHandler@41abee65 under GET@/v1/jobs/:jobid/vertices/:vertexid/taskmanagers.
2023-03-07 15:58:59  [ main:4422 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.JobVertexTaskManagersHandler@41abee65 under GET@/jobs/:jobid/vertices/:vertexid/taskmanagers.
2023-03-07 15:58:59  [ main:4422 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexWatermarksHandler@6fc6f68f under GET@/v1/jobs/:jobid/vertices/:vertexid/watermarks.
2023-03-07 15:58:59  [ main:4422 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexWatermarksHandler@6fc6f68f under GET@/jobs/:jobid/vertices/:vertexid/watermarks.
2023-03-07 15:58:59  [ main:4423 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@297dff3a under GET@/v1/jobs/:jobid/yarn-cancel.
2023-03-07 15:58:59  [ main:4423 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@297dff3a under GET@/jobs/:jobid/yarn-cancel.
2023-03-07 15:58:59  [ main:4423 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@30437e9c under GET@/v1/jobs/:jobid/yarn-stop.
2023-03-07 15:58:59  [ main:4423 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@30437e9c under GET@/jobs/:jobid/yarn-stop.
2023-03-07 15:58:59  [ main:4423 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler@2e057637 under GET@/v1/overview.
2023-03-07 15:58:59  [ main:4423 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler@2e057637 under GET@/overview.
2023-03-07 15:58:59  [ main:4423 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalTriggerHandler@762f8ff6 under POST@/v1/savepoint-disposal.
2023-03-07 15:58:59  [ main:4423 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalTriggerHandler@762f8ff6 under POST@/savepoint-disposal.
2023-03-07 15:58:59  [ main:4423 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalStatusHandler@30ed2a26 under GET@/v1/savepoint-disposal/:triggerid.
2023-03-07 15:58:59  [ main:4423 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalStatusHandler@30ed2a26 under GET@/savepoint-disposal/:triggerid.
2023-03-07 15:58:59  [ main:4423 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagersHandler@1bf10539 under GET@/v1/taskmanagers.
2023-03-07 15:58:59  [ main:4423 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagersHandler@1bf10539 under GET@/taskmanagers.
2023-03-07 15:58:59  [ main:4423 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingTaskManagersMetricsHandler@1e406cbe under GET@/v1/taskmanagers/metrics.
2023-03-07 15:58:59  [ main:4423 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingTaskManagersMetricsHandler@1e406cbe under GET@/taskmanagers/metrics.
2023-03-07 15:58:59  [ main:4423 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandler@141a10bf under GET@/v1/taskmanagers/:taskmanagerid.
2023-03-07 15:58:59  [ main:4423 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandler@141a10bf under GET@/taskmanagers/:taskmanagerid.
2023-03-07 15:58:59  [ main:4423 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogFileHandler@4dffa400 under GET@/v1/taskmanagers/:taskmanagerid/log.
2023-03-07 15:58:59  [ main:4423 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogFileHandler@4dffa400 under GET@/taskmanagers/:taskmanagerid/log.
2023-03-07 15:58:59  [ main:4423 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandler@5e34a84b under GET@/v1/taskmanagers/:taskmanagerid/logs.
2023-03-07 15:58:59  [ main:4423 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandler@5e34a84b under GET@/taskmanagers/:taskmanagerid/logs.
2023-03-07 15:58:59  [ main:4423 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerCustomLogHandler@438c0aaf under GET@/v1/taskmanagers/:taskmanagerid/logs/:filename.
2023-03-07 15:58:59  [ main:4423 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerCustomLogHandler@438c0aaf under GET@/taskmanagers/:taskmanagerid/logs/:filename.
2023-03-07 15:58:59  [ main:4423 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.metrics.TaskManagerMetricsHandler@7577589 under GET@/v1/taskmanagers/:taskmanagerid/metrics.
2023-03-07 15:58:59  [ main:4423 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.job.metrics.TaskManagerMetricsHandler@7577589 under GET@/taskmanagers/:taskmanagerid/metrics.
2023-03-07 15:58:59  [ main:4423 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerStdoutFileHandler@37b80ec7 under GET@/v1/taskmanagers/:taskmanagerid/stdout.
2023-03-07 15:58:59  [ main:4423 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerStdoutFileHandler@37b80ec7 under GET@/taskmanagers/:taskmanagerid/stdout.
2023-03-07 15:58:59  [ main:4423 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerThreadDumpHandler@4cb58e8b under GET@/v1/taskmanagers/:taskmanagerid/thread-dump.
2023-03-07 15:58:59  [ main:4423 ] - [ DEBUG ]  
Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerThreadDumpHandler@4cb58e8b under GET@/taskmanagers/:taskmanagerid/thread-dump.
2023-03-07 15:58:59  [ main:4428 ] - [ DEBUG ]  
-Dio.netty.eventLoopThreads: 32
2023-03-07 15:58:59  [ main:4446 ] - [ DEBUG ]  
-Dio.netty.noKeySetOptimization: false
2023-03-07 15:58:59  [ main:4446 ] - [ DEBUG ]  
-Dio.netty.selectorAutoRebuildThreshold: 512
2023-03-07 15:58:59  [ main:4451 ] - [ DEBUG ]  
org.jctools-core.MpscChunkedArrayQueue: available
2023-03-07 15:58:59  [ main:4485 ] - [ DEBUG ]  
-Dio.netty.processId: 20136 (auto-detected)
2023-03-07 15:58:59  [ main:4487 ] - [ DEBUG ]  
-Djava.net.preferIPv4Stack: false
2023-03-07 15:58:59  [ main:4487 ] - [ DEBUG ]  
-Djava.net.preferIPv6Addresses: false
2023-03-07 15:59:00  [ main:5075 ] - [ DEBUG ]  
Loopback interface: lo (Software Loopback Interface 1, 127.0.0.1)
2023-03-07 15:59:00  [ main:5076 ] - [ DEBUG ]  
Failed to get SOMAXCONN from sysctl and file \proc\sys\net\core\somaxconn. Default: 200
2023-03-07 15:59:00  [ main:5692 ] - [ DEBUG ]  
-Dio.netty.machineId: 70:66:55:ff:fe:09:13:3b (auto-detected)
2023-03-07 15:59:00  [ main:5701 ] - [ DEBUG ]  
-Dorg.apache.flink.shaded.netty4.io.netty.leakDetection.level: simple
2023-03-07 15:59:00  [ main:5701 ] - [ DEBUG ]  
-Dorg.apache.flink.shaded.netty4.io.netty.leakDetection.targetRecords: 4
2023-03-07 15:59:00  [ main:5721 ] - [ DEBUG ]  
-Dio.netty.allocator.numHeapArenas: 32
2023-03-07 15:59:00  [ main:5721 ] - [ DEBUG ]  
-Dio.netty.allocator.numDirectArenas: 32
2023-03-07 15:59:00  [ main:5721 ] - [ DEBUG ]  
-Dio.netty.allocator.pageSize: 8192
2023-03-07 15:59:00  [ main:5721 ] - [ DEBUG ]  
-Dio.netty.allocator.maxOrder: 11
2023-03-07 15:59:00  [ main:5721 ] - [ DEBUG ]  
-Dio.netty.allocator.chunkSize: 16777216
2023-03-07 15:59:00  [ main:5721 ] - [ DEBUG ]  
-Dio.netty.allocator.smallCacheSize: 256
2023-03-07 15:59:00  [ main:5721 ] - [ DEBUG ]  
-Dio.netty.allocator.normalCacheSize: 64
2023-03-07 15:59:00  [ main:5721 ] - [ DEBUG ]  
-Dio.netty.allocator.maxCachedBufferCapacity: 32768
2023-03-07 15:59:00  [ main:5721 ] - [ DEBUG ]  
-Dio.netty.allocator.cacheTrimInterval: 8192
2023-03-07 15:59:00  [ main:5722 ] - [ DEBUG ]  
-Dio.netty.allocator.cacheTrimIntervalMillis: 0
2023-03-07 15:59:00  [ main:5722 ] - [ DEBUG ]  
-Dio.netty.allocator.useCacheForAllThreads: true
2023-03-07 15:59:00  [ main:5722 ] - [ DEBUG ]  
-Dio.netty.allocator.maxCachedByteBuffersPerChunk: 1023
2023-03-07 15:59:00  [ main:5733 ] - [ DEBUG ]  
-Dio.netty.allocator.type: pooled
2023-03-07 15:59:00  [ main:5733 ] - [ DEBUG ]  
-Dio.netty.threadLocalDirectBufferSize: 0
2023-03-07 15:59:00  [ main:5733 ] - [ DEBUG ]  
-Dio.netty.maxThreadLocalCharBufferSize: 16384
2023-03-07 15:59:00  [ main:5746 ] - [ DEBUG ]  
Binding rest endpoint to null:0.
2023-03-07 15:59:00  [ main:5747 ] - [ INFO ]  
Rest endpoint listening at localhost:63115
2023-03-07 15:59:00  [ main:5748 ] - [ INFO ]  
Proposing leadership to contender http://localhost:63115
2023-03-07 15:59:00  [ mini-cluster-io-thread-1:5750 ] - [ INFO ]  
http://localhost:63115 was granted leadership with leaderSessionID=5096a0a3-6ff2-4c71-849f-fd976b2d40e7
2023-03-07 15:59:00  [ mini-cluster-io-thread-1:5751 ] - [ INFO ]  
Received confirmation of leadership for leader http://localhost:63115 , session=5096a0a3-6ff2-4c71-849f-fd976b2d40e7
2023-03-07 15:59:00  [ main:5760 ] - [ DEBUG ]  
Starting Dispatcher.
2023-03-07 15:59:00  [ main:5763 ] - [ INFO ]  
Proposing leadership to contender LeaderContender: DefaultDispatcherRunner
2023-03-07 15:59:00  [ main:5763 ] - [ DEBUG ]  
Starting ResourceManagerService.
2023-03-07 15:59:00  [ main:5763 ] - [ INFO ]  
Starting resource manager service.
2023-03-07 15:59:00  [ main:5763 ] - [ INFO ]  
Proposing leadership to contender LeaderContender: ResourceManagerServiceImpl
2023-03-07 15:59:00  [ mini-cluster-io-thread-2:5764 ] - [ INFO ]  
DefaultDispatcherRunner was granted leadership with leader id ccad85dd-3b38-4260-881f-cfb028ff6ad4. Creating new DispatcherLeaderProcess.
2023-03-07 15:59:00  [ pool-2-thread-1:5765 ] - [ INFO ]  
Resource manager service is granted leadership with session id 869d2009-e85e-4dea-ae9f-7e2ca9029db9.
2023-03-07 15:59:00  [ main:5767 ] - [ INFO ]  
Flink Mini Cluster started successfully
2023-03-07 15:59:00  [ mini-cluster-io-thread-2:5770 ] - [ INFO ]  
Start SessionDispatcherLeaderProcess.
2023-03-07 15:59:00  [ mini-cluster-io-thread-1:5771 ] - [ INFO ]  
Recover all persisted job graphs.
2023-03-07 15:59:00  [ mini-cluster-io-thread-1:5771 ] - [ INFO ]  
Successfully recovered 0 persisted job graphs.
2023-03-07 15:59:00  [ flink-akka.actor.supervisor-dispatcher-5:5779 ] - [ DEBUG ]  
Starting FencedAkkaRpcActor with name dispatcher_1.
2023-03-07 15:59:00  [ mini-cluster-io-thread-1:5780 ] - [ INFO ]  
Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_1 .
2023-03-07 15:59:00  [ flink-akka.actor.supervisor-dispatcher-5:5784 ] - [ DEBUG ]  
Starting FencedAkkaRpcActor with name resourcemanager_2.
2023-03-07 15:59:00  [ pool-2-thread-1:5784 ] - [ INFO ]  
Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_2 .
2023-03-07 15:59:00  [ mini-cluster-io-thread-1:5790 ] - [ INFO ]  
Received confirmation of leadership for leader akka://flink/user/rpc/dispatcher_1 , session=ccad85dd-3b38-4260-881f-cfb028ff6ad4
2023-03-07 15:59:00  [ mini-cluster-io-thread-3:5792 ] - [ DEBUG ]  
Try to connect to remote RPC endpoint with address akka://flink/user/rpc/dispatcher_1. Returning a org.apache.flink.runtime.dispatcher.DispatcherGateway gateway.
2023-03-07 15:59:00  [ mini-cluster-io-thread-4:5792 ] - [ DEBUG ]  
Try to connect to remote RPC endpoint with address akka://flink/user/rpc/dispatcher_1. Returning a org.apache.flink.runtime.dispatcher.DispatcherGateway gateway.
2023-03-07 15:59:00  [ flink-akka.actor.default-dispatcher-4:5792 ] - [ INFO ]  
Starting the resource manager.
2023-03-07 15:59:00  [ flink-akka.actor.default-dispatcher-4:5797 ] - [ DEBUG ]  
Starting the slot manager.
2023-03-07 15:59:00  [ flink-akka.actor.default-dispatcher-4:5804 ] - [ DEBUG ]  
Trigger heartbeat request.
2023-03-07 15:59:00  [ mini-cluster-io-thread-2:5804 ] - [ INFO ]  
Received confirmation of leadership for leader akka://flink/user/rpc/resourcemanager_2 , session=869d2009-e85e-4dea-ae9f-7e2ca9029db9
2023-03-07 15:59:00  [ flink-akka.actor.default-dispatcher-4:5804 ] - [ DEBUG ]  
Trigger heartbeat request.
2023-03-07 15:59:00  [ mini-cluster-io-thread-2:5804 ] - [ DEBUG ]  
Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_2. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
2023-03-07 15:59:00  [ mini-cluster-io-thread-1:5804 ] - [ DEBUG ]  
Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_2. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
2023-03-07 15:59:00  [ flink-akka.actor.default-dispatcher-7:5826 ] - [ INFO ]  
Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_2(ae9f7e2ca9029db9869d2009e85e4dea).
2023-03-07 15:59:00  [ flink-akka.actor.default-dispatcher-7:5833 ] - [ DEBUG ]  
Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_2. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
2023-03-07 15:59:00  [ flink-akka.actor.default-dispatcher-8:5834 ] - [ INFO ]  
Resolved ResourceManager address, beginning registration
2023-03-07 15:59:00  [ flink-akka.actor.default-dispatcher-8:5835 ] - [ DEBUG ]  
Registration at ResourceManager attempt 1 (timeout=100ms)
2023-03-07 15:59:00  [ flink-akka.actor.default-dispatcher-6:5835 ] - [ INFO ]  
Received JobGraph submission 'collect' (2df91b98749e65938b01d740ad2b1ab1).
2023-03-07 15:59:00  [ flink-akka.actor.default-dispatcher-7:5835 ] - [ DEBUG ]  
Try to connect to remote RPC endpoint with address akka://flink/user/rpc/taskmanager_0. Returning a org.apache.flink.runtime.taskexecutor.TaskExecutorGateway gateway.
2023-03-07 15:59:00  [ flink-akka.actor.default-dispatcher-6:5836 ] - [ INFO ]  
Submitting job 'collect' (2df91b98749e65938b01d740ad2b1ab1).
2023-03-07 15:59:00  [ flink-akka.actor.default-dispatcher-4:5838 ] - [ INFO ]  
Registering TaskManager with ResourceID e6aa20be-363d-4d66-b022-cceae3fbad81 (akka://flink/user/rpc/taskmanager_0) at ResourceManager
2023-03-07 15:59:00  [ flink-akka.actor.default-dispatcher-7:5839 ] - [ DEBUG ]  
Registration with ResourceManager at akka://flink/user/rpc/resourcemanager_2 was successful.
2023-03-07 15:59:00  [ flink-akka.actor.default-dispatcher-8:5839 ] - [ INFO ]  
Successful registration at resource manager akka://flink/user/rpc/resourcemanager_2 under registration id 5245087c109dd05066a9e0411da0019d.
2023-03-07 15:59:00  [ flink-akka.actor.default-dispatcher-7:5841 ] - [ DEBUG ]  
Registering task executor e6aa20be-363d-4d66-b022-cceae3fbad81 under 5245087c109dd05066a9e0411da0019d at the slot manager.
2023-03-07 15:59:00  [ flink-akka.actor.default-dispatcher-6:5850 ] - [ DEBUG ]  
Start leadership runner for job 2df91b98749e65938b01d740ad2b1ab1.
2023-03-07 15:59:00  [ flink-akka.actor.default-dispatcher-6:5850 ] - [ INFO ]  
Proposing leadership to contender LeaderContender: JobMasterServiceLeadershipRunner
2023-03-07 15:59:00  [ mini-cluster-io-thread-1:5851 ] - [ DEBUG ]  
Create new JobMasterServiceProcess because we were granted leadership under 7b35f068-73e5-418a-bc92-6b582ba1e6c3.
2023-03-07 15:59:00  [ ForkJoinPool.commonPool-worker-11:5853 ] - [ DEBUG ]  
Wait until job initialization is finished
2023-03-07 15:59:00  [ flink-akka.actor.supervisor-dispatcher-5:5863 ] - [ DEBUG ]  
Starting FencedAkkaRpcActor with name jobmanager_3.
2023-03-07 15:59:00  [ jobmanager-io-thread-1:5863 ] - [ INFO ]  
Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_3 .
2023-03-07 15:59:00  [ jobmanager-io-thread-1:5869 ] - [ INFO ]  
Initializing job 'collect' (2df91b98749e65938b01d740ad2b1ab1).
2023-03-07 15:59:00  [ jobmanager-io-thread-1:5885 ] - [ INFO ]  
Using restart back off time strategy NoRestartBackoffTimeStrategy for collect (2df91b98749e65938b01d740ad2b1ab1).
2023-03-07 15:59:00  [ jobmanager-io-thread-1:5918 ] - [ INFO ]  
Running initialization on master for job collect (2df91b98749e65938b01d740ad2b1ab1).
2023-03-07 15:59:00  [ jobmanager-io-thread-1:5919 ] - [ INFO ]  
Successfully ran initialization on master in 0 ms.
2023-03-07 15:59:00  [ jobmanager-io-thread-1:5919 ] - [ DEBUG ]  
Adding 2 vertices from job graph collect (2df91b98749e65938b01d740ad2b1ab1).
2023-03-07 15:59:00  [ jobmanager-io-thread-1:5919 ] - [ DEBUG ]  
Attaching 2 topologically sorted vertices to existing job graph with 0 vertices and 0 intermediate results.
2023-03-07 15:59:00  [ jobmanager-io-thread-1:5928 ] - [ DEBUG ]  
Connecting ExecutionJobVertex cbc357ccb763df2852fee8c4fc7d55f2 (Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time])) to 0 predecessors.
2023-03-07 15:59:00  [ jobmanager-io-thread-1:5934 ] - [ DEBUG ]  
Connecting ExecutionJobVertex 337adade1e207453ed3502e01d75fd03 (OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink) to 1 predecessors.
2023-03-07 15:59:00  [ jobmanager-io-thread-1:5934 ] - [ DEBUG ]  
Connecting input 0 of vertex 337adade1e207453ed3502e01d75fd03 (OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink) to intermediate result referenced via predecessor cbc357ccb763df2852fee8c4fc7d55f2 (Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time])).
2023-03-07 15:59:00  [ jobmanager-io-thread-1:5946 ] - [ INFO ]  
Built 1 pipelined regions in 0 ms
2023-03-07 15:59:00  [ jobmanager-io-thread-1:5949 ] - [ DEBUG ]  
Successfully created execution graph from job graph collect (2df91b98749e65938b01d740ad2b1ab1).
2023-03-07 15:59:00  [ jobmanager-io-thread-1:5984 ] - [ INFO ]  
No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@2d6f6f7c
2023-03-07 15:59:00  [ jobmanager-io-thread-1:5984 ] - [ INFO ]  
State backend loader loads the state backend as HashMapStateBackend
2023-03-07 15:59:00  [ jobmanager-io-thread-1:5985 ] - [ DEBUG ]  
The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
2023-03-07 15:59:00  [ jobmanager-io-thread-1:5986 ] - [ INFO ]  
Checkpoint storage is set to 'jobmanager'
2023-03-07 15:59:00  [ jobmanager-io-thread-1:6005 ] - [ DEBUG ]  
Status of the shared state registry of job 2df91b98749e65938b01d740ad2b1ab1 after restore: SharedStateRegistry{registeredStates={}}.
2023-03-07 15:59:00  [ jobmanager-io-thread-1:6005 ] - [ INFO ]  
No checkpoint found during restore.
2023-03-07 15:59:00  [ jobmanager-io-thread-1:6005 ] - [ DEBUG ]  
Resetting the master hooks.
2023-03-07 15:59:00  [ jobmanager-io-thread-1:6015 ] - [ INFO ]  
Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@22744cfb for collect (2df91b98749e65938b01d740ad2b1ab1).
2023-03-07 15:59:01  [ jobmanager-io-thread-1:6026 ] - [ DEBUG ]  
Successfully created the JobMasterService for job 2df91b98749e65938b01d740ad2b1ab1 under leader id 7b35f068-73e5-418a-bc92-6b582ba1e6c3.
2023-03-07 15:59:01  [ jobmanager-io-thread-1:6026 ] - [ DEBUG ]  
Confirm leadership 7b35f068-73e5-418a-bc92-6b582ba1e6c3.
2023-03-07 15:59:01  [ jobmanager-io-thread-1:6027 ] - [ INFO ]  
Received confirmation of leadership for leader akka://flink/user/rpc/jobmanager_3 , session=7b35f068-73e5-418a-bc92-6b582ba1e6c3
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-7:6030 ] - [ INFO ]  
Starting execution of job 'collect' (2df91b98749e65938b01d740ad2b1ab1) under job master id bc926b582ba1e6c37b35f06873e5418a.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-7:6031 ] - [ INFO ]  
Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-7:6032 ] - [ INFO ]  
Job collect (2df91b98749e65938b01d740ad2b1ab1) switched from state CREATED to RUNNING.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-7:6037 ] - [ INFO ]  
Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1) (9d55e275358a411b21f20ae97427e4a6) switched from CREATED to SCHEDULED.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-7:6037 ] - [ INFO ]  
OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1) (a2d74a9e1b54d9b31d4b8c09a2b51b2a) switched from CREATED to SCHEDULED.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-7:6044 ] - [ DEBUG ]  
Received slot request [SlotRequestId{0b9c87f1d208bc6c26743de7e1662e03}] with resource requirements: ResourceProfile{UNKNOWN}
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-7:6047 ] - [ DEBUG ]  
Request new allocated slot with slot request id SlotRequestId{0b9c87f1d208bc6c26743de7e1662e03} and resource profile ResourceProfile{UNKNOWN}
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-7:6049 ] - [ DEBUG ]  
Declare new resource requirements for job 2df91b98749e65938b01d740ad2b1ab1.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
	acquired resources: ResourceCounter{resources={}}
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-7:6051 ] - [ DEBUG ]  
Request a logical slot (SlotRequestId{468f0992dbfb40bcc3fb8705f9623220}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_0) from the physical slot (SlotRequestId{0b9c87f1d208bc6c26743de7e1662e03})
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-7:6053 ] - [ DEBUG ]  
Request a logical slot (SlotRequestId{0956687a0eecd8b0ac2a6a1699ffe356}) for execution vertex (id 337adade1e207453ed3502e01d75fd03_0) from the physical slot (SlotRequestId{0b9c87f1d208bc6c26743de7e1662e03})
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-7:6057 ] - [ DEBUG ]  
Trigger heartbeat request.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-7:6057 ] - [ INFO ]  
Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_2(ae9f7e2ca9029db9869d2009e85e4dea)
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-7:6058 ] - [ DEBUG ]  
Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_2. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-7:6060 ] - [ INFO ]  
Resolved ResourceManager address, beginning registration
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-7:6060 ] - [ DEBUG ]  
Registration at ResourceManager attempt 1 (timeout=100ms)
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-6:6060 ] - [ DEBUG ]  
Add job 2df91b98749e65938b01d740ad2b1ab1 to job leader id monitoring.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-6:6062 ] - [ INFO ]  
Registering job manager bc926b582ba1e6c37b35f06873e5418a@akka://flink/user/rpc/jobmanager_3 for job 2df91b98749e65938b01d740ad2b1ab1.
2023-03-07 15:59:01  [ mini-cluster-io-thread-4:6062 ] - [ DEBUG ]  
Job 2df91b98749e65938b01d740ad2b1ab1 has a new job leader 7b35f068-73e5-418a-bc92-6b582ba1e6c3@akka://flink/user/rpc/jobmanager_3.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-6:6063 ] - [ DEBUG ]  
Try to connect to remote RPC endpoint with address akka://flink/user/rpc/jobmanager_3. Returning a org.apache.flink.runtime.jobmaster.JobMasterGateway gateway.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-7:6066 ] - [ INFO ]  
Registered job manager bc926b582ba1e6c37b35f06873e5418a@akka://flink/user/rpc/jobmanager_3 for job 2df91b98749e65938b01d740ad2b1ab1.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-6:6067 ] - [ DEBUG ]  
Registration with ResourceManager at akka://flink/user/rpc/resourcemanager_2 was successful.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-7:6068 ] - [ INFO ]  
JobManager successfully registered at ResourceManager, leader id: ae9f7e2ca9029db9869d2009e85e4dea.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-6:6069 ] - [ INFO ]  
Received resource requirements from job 2df91b98749e65938b01d740ad2b1ab1: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-6:6070 ] - [ DEBUG ]  
Initiating tracking of resources for job 2df91b98749e65938b01d740ad2b1ab1.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-6:6072 ] - [ DEBUG ]  
Starting allocation of slot e6aa20be-363d-4d66-b022-cceae3fbad81_0 for job 2df91b98749e65938b01d740ad2b1ab1 with resource profile ResourceProfile{UNKNOWN}.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-7:6074 ] - [ INFO ]  
Receive slot request c4ad44bd4da65edb95f6eee66ad62572 for job 2df91b98749e65938b01d740ad2b1ab1 from resource manager with leader id ae9f7e2ca9029db9869d2009e85e4dea.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-7:6079 ] - [ DEBUG ]  
Initialized MemoryManager with total memory size 134217728 and page size 32768.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-7:6079 ] - [ INFO ]  
Allocated slot for c4ad44bd4da65edb95f6eee66ad62572.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-7:6080 ] - [ INFO ]  
Add job 2df91b98749e65938b01d740ad2b1ab1 for job leader monitoring.
2023-03-07 15:59:01  [ mini-cluster-io-thread-1:6082 ] - [ DEBUG ]  
New leader information for job 2df91b98749e65938b01d740ad2b1ab1. Address: akka://flink/user/rpc/jobmanager_3, leader id: bc926b582ba1e6c37b35f06873e5418a.
2023-03-07 15:59:01  [ mini-cluster-io-thread-1:6083 ] - [ INFO ]  
Try to register at job manager akka://flink/user/rpc/jobmanager_3 with leader id 7b35f068-73e5-418a-bc92-6b582ba1e6c3.
2023-03-07 15:59:01  [ mini-cluster-io-thread-1:6084 ] - [ DEBUG ]  
Try to connect to remote RPC endpoint with address akka://flink/user/rpc/jobmanager_3. Returning a org.apache.flink.runtime.jobmaster.JobMasterGateway gateway.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-7:6085 ] - [ INFO ]  
Resolved JobManager address, beginning registration
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-7:6085 ] - [ DEBUG ]  
Registration at JobManager attempt 1 (timeout=100ms)
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-8:6087 ] - [ DEBUG ]  
Try to connect to remote RPC endpoint with address akka://flink/user/rpc/taskmanager_0. Returning a org.apache.flink.runtime.taskexecutor.TaskExecutorGateway gateway.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-8:6087 ] - [ DEBUG ]  
Register new TaskExecutor e6aa20be-363d-4d66-b022-cceae3fbad81.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-8:6088 ] - [ DEBUG ]  
Registration with JobManager at akka://flink/user/rpc/jobmanager_3 was successful.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-8:6089 ] - [ INFO ]  
Successful registration at job manager akka://flink/user/rpc/jobmanager_3 for job 2df91b98749e65938b01d740ad2b1ab1.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-8:6090 ] - [ INFO ]  
Establish JobManager connection for job 2df91b98749e65938b01d740ad2b1ab1.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-8:6093 ] - [ INFO ]  
Offer reserved slots to the leader of job 2df91b98749e65938b01d740ad2b1ab1.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-6:6096 ] - [ DEBUG ]  
Received 1 slot offers from TaskExecutor e6aa20be-363d-4d66-b022-cceae3fbad81 @ 127.0.0.1 (dataPort=-1).
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-6:6097 ] - [ DEBUG ]  
Matched slot offer c4ad44bd4da65edb95f6eee66ad62572 to requirement ResourceProfile{UNKNOWN}.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-6:6098 ] - [ DEBUG ]  
Acquired new resources; new total acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=1}}
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-6:6099 ] - [ DEBUG ]  
Matched slot AllocatedSlot c4ad44bd4da65edb95f6eee66ad62572 @ e6aa20be-363d-4d66-b022-cceae3fbad81 @ 127.0.0.1 (dataPort=-1) - 0 to pending request PendingRequest{slotRequestId=SlotRequestId{0b9c87f1d208bc6c26743de7e1662e03}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=false, unfulfillableSince=9223372036854775807}.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-6:6099 ] - [ DEBUG ]  
Reserve slot c4ad44bd4da65edb95f6eee66ad62572 for slot request id SlotRequestId{0b9c87f1d208bc6c26743de7e1662e03}
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-6:6099 ] - [ DEBUG ]  
Reserve free slot with allocation id c4ad44bd4da65edb95f6eee66ad62572.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-6:6100 ] - [ DEBUG ]  
Allocated logical slot (SlotRequestId{468f0992dbfb40bcc3fb8705f9623220}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_0) from the physical slot (SlotRequestId{0b9c87f1d208bc6c26743de7e1662e03})
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-6:6104 ] - [ DEBUG ]  
Allocated logical slot (SlotRequestId{0956687a0eecd8b0ac2a6a1699ffe356}) for execution vertex (id 337adade1e207453ed3502e01d75fd03_0) from the physical slot (SlotRequestId{0b9c87f1d208bc6c26743de7e1662e03})
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-6:6105 ] - [ INFO ]  
Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1) (9d55e275358a411b21f20ae97427e4a6) switched from SCHEDULED to DEPLOYING.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-6:6105 ] - [ INFO ]  
Deploying Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1) (attempt #0) with attempt id 9d55e275358a411b21f20ae97427e4a6 to e6aa20be-363d-4d66-b022-cceae3fbad81 @ 127.0.0.1 (dataPort=-1) with allocation id c4ad44bd4da65edb95f6eee66ad62572
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-6:6111 ] - [ INFO ]  
OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1) (a2d74a9e1b54d9b31d4b8c09a2b51b2a) switched from SCHEDULED to DEPLOYING.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-6:6111 ] - [ INFO ]  
Deploying OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1) (attempt #0) with attempt id a2d74a9e1b54d9b31d4b8c09a2b51b2a to e6aa20be-363d-4d66-b022-cceae3fbad81 @ 127.0.0.1 (dataPort=-1) with allocation id c4ad44bd4da65edb95f6eee66ad62572
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-8:6111 ] - [ INFO ]  
Activate slot c4ad44bd4da65edb95f6eee66ad62572.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-8:6122 ] - [ DEBUG ]  
Registered new allocation id c4ad44bd4da65edb95f6eee66ad62572 for local state stores for job 2df91b98749e65938b01d740ad2b1ab1.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-8:6125 ] - [ DEBUG ]  
Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[C:\Users\ADMINI~1\AppData\Local\Temp\localState\aid_c4ad44bd4da65edb95f6eee66ad62572], jobID=2df91b98749e65938b01d740ad2b1ab1, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=0}} for 2df91b98749e65938b01d740ad2b1ab1 - cbc357ccb763df2852fee8c4fc7d55f2 - 0 under allocation id c4ad44bd4da65edb95f6eee66ad62572.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-8:6129 ] - [ INFO ]  
StateChangelogStorageLoader initialized with shortcut names {memory}.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-8:6129 ] - [ INFO ]  
Creating a changelog storage with name 'memory'.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-8:6130 ] - [ DEBUG ]  
Registered new state changelog storage for job 2df91b98749e65938b01d740ad2b1ab1 : org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorage@2ba4b7d.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-8:6143 ] - [ DEBUG ]  
Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0 (9d55e275358a411b21f20ae97427e4a6): Initialized org.apache.flink.runtime.io.network.partition.ResultPartitionFactory@57b2098a
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-8:6148 ] - [ INFO ]  
Received task Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0 (9d55e275358a411b21f20ae97427e4a6), deploy into slot with allocation id c4ad44bd4da65edb95f6eee66ad62572.
2023-03-07 15:59:01  [ Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0:6148 ] - [ INFO ]  
Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0 (9d55e275358a411b21f20ae97427e4a6) switched from CREATED to DEPLOYING.
2023-03-07 15:59:01  [ Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0:6149 ] - [ DEBUG ]  
Creating FileSystem stream leak safety net for task Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0 (9d55e275358a411b21f20ae97427e4a6) [DEPLOYING]
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-8:6150 ] - [ INFO ]  
Activate slot c4ad44bd4da65edb95f6eee66ad62572.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-8:6152 ] - [ DEBUG ]  
Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[C:\Users\ADMINI~1\AppData\Local\Temp\localState\aid_c4ad44bd4da65edb95f6eee66ad62572], jobID=2df91b98749e65938b01d740ad2b1ab1, jobVertexID=337adade1e207453ed3502e01d75fd03, subtaskIndex=0}} for 2df91b98749e65938b01d740ad2b1ab1 - 337adade1e207453ed3502e01d75fd03 - 0 under allocation id c4ad44bd4da65edb95f6eee66ad62572.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-8:6152 ] - [ DEBUG ]  
Found existing state changelog storage for job 2df91b98749e65938b01d740ad2b1ab1: org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorage@2ba4b7d.
2023-03-07 15:59:01  [ Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0:6154 ] - [ INFO ]  
Loading JAR files for task Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0 (9d55e275358a411b21f20ae97427e4a6) [DEPLOYING].
2023-03-07 15:59:01  [ Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0:6155 ] - [ DEBUG ]  
Getting user code class loader for task 9d55e275358a411b21f20ae97427e4a6 at library cache manager took 0 milliseconds
2023-03-07 15:59:01  [ Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0:6156 ] - [ DEBUG ]  
Registering task at network: Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0 (9d55e275358a411b21f20ae97427e4a6) [DEPLOYING].
2023-03-07 15:59:01  [ Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0:6159 ] - [ DEBUG ]  
Using a local buffer pool with 2-10 buffers
2023-03-07 15:59:01  [ Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0:6159 ] - [ DEBUG ]  
Registered PipelinedResultPartition 65f51bb133c7c1301453f2c303fab195#0@9d55e275358a411b21f20ae97427e4a6 [PIPELINED_BOUNDED, 1 subpartitions, 2 pending consumptions].
2023-03-07 15:59:01  [ Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0:6160 ] - [ DEBUG ]  
registering 65f51bb133c7c1301453f2c303fab195#0@9d55e275358a411b21f20ae97427e4a6
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-8:6164 ] - [ DEBUG ]  
OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0 (a2d74a9e1b54d9b31d4b8c09a2b51b2a): Created 1 input channels (local: 1, remote: 0, unknown: 0).
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-8:6166 ] - [ INFO ]  
Received task OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0 (a2d74a9e1b54d9b31d4b8c09a2b51b2a), deploy into slot with allocation id c4ad44bd4da65edb95f6eee66ad62572.
2023-03-07 15:59:01  [ OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0:6166 ] - [ INFO ]  
OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0 (a2d74a9e1b54d9b31d4b8c09a2b51b2a) switched from CREATED to DEPLOYING.
2023-03-07 15:59:01  [ OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0:6167 ] - [ DEBUG ]  
Creating FileSystem stream leak safety net for task OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0 (a2d74a9e1b54d9b31d4b8c09a2b51b2a) [DEPLOYING]
2023-03-07 15:59:01  [ OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0:6168 ] - [ INFO ]  
Loading JAR files for task OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0 (a2d74a9e1b54d9b31d4b8c09a2b51b2a) [DEPLOYING].
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-8:6168 ] - [ INFO ]  
Activate slot c4ad44bd4da65edb95f6eee66ad62572.
2023-03-07 15:59:01  [ OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0:6168 ] - [ DEBUG ]  
Getting user code class loader for task a2d74a9e1b54d9b31d4b8c09a2b51b2a at library cache manager took 0 milliseconds
2023-03-07 15:59:01  [ OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0:6169 ] - [ DEBUG ]  
Registering task at network: OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0 (a2d74a9e1b54d9b31d4b8c09a2b51b2a) [DEPLOYING].
2023-03-07 15:59:01  [ OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0:6170 ] - [ DEBUG ]  
Using a local buffer pool with 1-8 buffers
2023-03-07 15:59:01  [ Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0:6185 ] - [ DEBUG ]  
Using partitioner HASH for output 0 of task Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time])
2023-03-07 15:59:01  [ OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0:6186 ] - [ INFO ]  
No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@78feedef
2023-03-07 15:59:01  [ OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0:6186 ] - [ INFO ]  
State backend loader loads the state backend as HashMapStateBackend
2023-03-07 15:59:01  [ OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0:6186 ] - [ DEBUG ]  
The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
2023-03-07 15:59:01  [ OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0:6187 ] - [ INFO ]  
Checkpoint storage is set to 'jobmanager'
2023-03-07 15:59:01  [ Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0:6190 ] - [ INFO ]  
No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@10b1b906
2023-03-07 15:59:01  [ Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0:6190 ] - [ INFO ]  
State backend loader loads the state backend as HashMapStateBackend
2023-03-07 15:59:01  [ Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0:6190 ] - [ DEBUG ]  
The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
2023-03-07 15:59:01  [ Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0:6190 ] - [ INFO ]  
Checkpoint storage is set to 'jobmanager'
2023-03-07 15:59:01  [ OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0:6195 ] - [ INFO ]  
OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0 (a2d74a9e1b54d9b31d4b8c09a2b51b2a) switched from DEPLOYING to INITIALIZING.
2023-03-07 15:59:01  [ Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0:6196 ] - [ INFO ]  
Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0 (9d55e275358a411b21f20ae97427e4a6) switched from DEPLOYING to INITIALIZING.
2023-03-07 15:59:01  [ OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0:6196 ] - [ DEBUG ]  
Initializing OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0.
2023-03-07 15:59:01  [ Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0:6197 ] - [ DEBUG ]  
Initializing Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-8:6197 ] - [ INFO ]  
OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1) (a2d74a9e1b54d9b31d4b8c09a2b51b2a) switched from DEPLOYING to INITIALIZING.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-8:6198 ] - [ INFO ]  
Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1) (9d55e275358a411b21f20ae97427e4a6) switched from DEPLOYING to INITIALIZING.
2023-03-07 15:59:01  [ Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0:6224 ] - [ DEBUG ]  
Compiling: StreamExecCalc$9 

 Code:

      public class StreamExecCalc$9 extends org.apache.flink.table.runtime.operators.TableStreamOperator
          implements org.apache.flink.streaming.api.operators.OneInputStreamOperator {

        private final Object[] references;
        org.apache.flink.table.data.BoxedWrapperRowData out = new org.apache.flink.table.data.BoxedWrapperRowData(3);
        private final org.apache.flink.streaming.runtime.streamrecord.StreamRecord outElement = new org.apache.flink.streaming.runtime.streamrecord.StreamRecord(null);

        public StreamExecCalc$9(
            Object[] references,
            org.apache.flink.streaming.runtime.tasks.StreamTask task,
            org.apache.flink.streaming.api.graph.StreamConfig config,
            org.apache.flink.streaming.api.operators.Output output,
            org.apache.flink.streaming.runtime.tasks.ProcessingTimeService processingTimeService) throws Exception {
          this.references = references;
          
          this.setup(task, config, output);
          if (this instanceof org.apache.flink.streaming.api.operators.AbstractStreamOperator) {
            ((org.apache.flink.streaming.api.operators.AbstractStreamOperator) this)
              .setProcessingTimeService(processingTimeService);
          }
        }

        @Override
        public void open() throws Exception {
          super.open();
          
        }

        @Override
        public void processElement(org.apache.flink.streaming.runtime.streamrecord.StreamRecord element) throws Exception {
          org.apache.flink.table.data.RowData in1 = (org.apache.flink.table.data.RowData) element.getValue();
          
          int field$6;
          boolean isNull$6;
          double field$7;
          boolean isNull$7;
          org.apache.flink.table.data.TimestampData field$8;
          boolean isNull$8;
          
          
          isNull$7 = in1.isNullAt(2);
          field$7 = -1.0d;
          if (!isNull$7) {
            field$7 = in1.getDouble(2);
          }
          isNull$6 = in1.isNullAt(0);
          field$6 = -1;
          if (!isNull$6) {
            field$6 = in1.getInt(0);
          }
          isNull$8 = in1.isNullAt(3);
          field$8 = null;
          if (!isNull$8) {
            field$8 = in1.getTimestamp(3, 3);
          }
          
          out.setRowKind(in1.getRowKind());
          
          
          
          
          if (isNull$6) {
            out.setNullAt(0);
          } else {
            out.setInt(0, field$6);
          }
                    
          
          
          if (isNull$7) {
            out.setNullAt(1);
          } else {
            out.setDouble(1, field$7);
          }
                    
          
          
          if (isNull$8) {
            out.setNullAt(2);
          } else {
            out.setNonPrimitiveValue(2, field$8);
          }
                    
                  
          output.collect(outElement.replace(out));
          
          
        }

        

        @Override
        public void close() throws Exception {
           super.close();
          
        }

        
      }
    
2023-03-07 15:59:01  [ OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0:6224 ] - [ DEBUG ]  
Compiling: StreamExecCalc$58 

 Code:

      public class StreamExecCalc$58 extends org.apache.flink.table.runtime.operators.TableStreamOperator
          implements org.apache.flink.streaming.api.operators.OneInputStreamOperator {

        private final Object[] references;
        org.apache.flink.table.data.BoxedWrapperRowData out = new org.apache.flink.table.data.BoxedWrapperRowData(3);
        private final org.apache.flink.streaming.runtime.streamrecord.StreamRecord outElement = new org.apache.flink.streaming.runtime.streamrecord.StreamRecord(null);

        public StreamExecCalc$58(
            Object[] references,
            org.apache.flink.streaming.runtime.tasks.StreamTask task,
            org.apache.flink.streaming.api.graph.StreamConfig config,
            org.apache.flink.streaming.api.operators.Output output,
            org.apache.flink.streaming.runtime.tasks.ProcessingTimeService processingTimeService) throws Exception {
          this.references = references;
          
          this.setup(task, config, output);
          if (this instanceof org.apache.flink.streaming.api.operators.AbstractStreamOperator) {
            ((org.apache.flink.streaming.api.operators.AbstractStreamOperator) this)
              .setProcessingTimeService(processingTimeService);
          }
        }

        @Override
        public void open() throws Exception {
          super.open();
          
        }

        @Override
        public void processElement(org.apache.flink.streaming.runtime.streamrecord.StreamRecord element) throws Exception {
          org.apache.flink.table.data.RowData in1 = (org.apache.flink.table.data.RowData) element.getValue();
          
          int field$49;
          boolean isNull$49;
          double field$50;
          boolean isNull$50;
          long field$51;
          boolean isNull$51;
          boolean isNull$52;
          boolean result$53;
          double field$54;
          boolean isNull$54;
          boolean isNull$56;
          double result$57;
          
          
          isNull$54 = in1.isNullAt(5);
          field$54 = -1.0d;
          if (!isNull$54) {
            field$54 = in1.getDouble(5);
          }
          isNull$49 = in1.isNullAt(0);
          field$49 = -1;
          if (!isNull$49) {
            field$49 = in1.getInt(0);
          }
          isNull$51 = in1.isNullAt(4);
          field$51 = -1L;
          if (!isNull$51) {
            field$51 = in1.getLong(4);
          }
          isNull$50 = in1.isNullAt(3);
          field$50 = -1.0d;
          if (!isNull$50) {
            field$50 = in1.getDouble(3);
          }
          
          out.setRowKind(in1.getRowKind());
          
          
          
          
          if (isNull$49) {
            out.setNullAt(0);
          } else {
            out.setInt(0, field$49);
          }
                    
          
          
          if (isNull$50) {
            out.setNullAt(1);
          } else {
            out.setDouble(1, field$50);
          }
                    
          
          
          isNull$52 = isNull$51 || false;
          result$53 = false;
          if (!isNull$52) {
            
            result$53 = field$51 > ((long) 0L);
            
          }
          
          double result$55 = -1.0d;
          boolean isNull$55;
          if (result$53) {
            
            isNull$55 = isNull$54;
            if (!isNull$55) {
              result$55 = field$54;
            }
          }
          else {
            
            isNull$55 = true;
            if (!isNull$55) {
              result$55 = ((double) -1.0d);
            }
          }
          
          isNull$56 = isNull$55 || isNull$51;
          result$57 = -1.0d;
          if (!isNull$56) {
            
            result$57 = (double) (result$55 / (new java.lang.Long(field$51)).doubleValue());
            
          }
          
          if (isNull$56) {
            out.setNullAt(2);
          } else {
            out.setDouble(2, result$57);
          }
                    
                  
          output.collect(outElement.replace(out));
          
          
        }

        

        @Override
        public void close() throws Exception {
           super.close();
          
        }

        
      }
    
2023-03-07 15:59:01  [ OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0:6248 ] - [ WARN ]  
The operator name Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) exceeded the 80 characters length limit and was truncated.
2023-03-07 15:59:01  [ OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0:6250 ] - [ WARN ]  
The operator name OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) exceeded the 80 characters length limit and was truncated.
2023-03-07 15:59:01  [ Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0:6252 ] - [ DEBUG ]  
Compiling: SourceConversion$5 

 Code:

      public class SourceConversion$5 extends org.apache.flink.table.runtime.operators.TableStreamOperator
          implements org.apache.flink.streaming.api.operators.OneInputStreamOperator {

        private final Object[] references;
        private transient org.apache.flink.table.data.util.DataFormatConverters.PojoConverter converter$0;
        org.apache.flink.table.data.GenericRowData out = new org.apache.flink.table.data.GenericRowData(4);
        private final org.apache.flink.streaming.runtime.streamrecord.StreamRecord outElement = new org.apache.flink.streaming.runtime.streamrecord.StreamRecord(null);

        public SourceConversion$5(
            Object[] references,
            org.apache.flink.streaming.runtime.tasks.StreamTask task,
            org.apache.flink.streaming.api.graph.StreamConfig config,
            org.apache.flink.streaming.api.operators.Output output,
            org.apache.flink.streaming.runtime.tasks.ProcessingTimeService processingTimeService) throws Exception {
          this.references = references;
          converter$0 = (((org.apache.flink.table.data.util.DataFormatConverters.PojoConverter) references[0]));
          this.setup(task, config, output);
          if (this instanceof org.apache.flink.streaming.api.operators.AbstractStreamOperator) {
            ((org.apache.flink.streaming.api.operators.AbstractStreamOperator) this)
              .setProcessingTimeService(processingTimeService);
          }
        }

        @Override
        public void open() throws Exception {
          super.open();
          
        }

        @Override
        public void processElement(org.apache.flink.streaming.runtime.streamrecord.StreamRecord element) throws Exception {
          org.apache.flink.table.data.RowData in1 = (org.apache.flink.table.data.RowData) (org.apache.flink.table.data.RowData) converter$0.toInternal((org.fllik.geektime.flinksql.Userproduct) element.getValue());
          
          int field$1;
          boolean isNull$1;
          org.apache.flink.table.data.binary.BinaryStringData field$2;
          boolean isNull$2;
          double field$3;
          boolean isNull$3;
          org.apache.flink.table.data.TimestampData result$4;
          boolean isNull$4;
          Long timestamp$4;
          isNull$3 = in1.isNullAt(2);
          field$3 = -1.0d;
          if (!isNull$3) {
            field$3 = in1.getDouble(2);
          }
          isNull$2 = in1.isNullAt(0);
          field$2 = org.apache.flink.table.data.binary.BinaryStringData.EMPTY_UTF8;
          if (!isNull$2) {
            field$2 = ((org.apache.flink.table.data.binary.BinaryStringData) in1.getString(0));
          }
          isNull$1 = in1.isNullAt(3);
          field$1 = -1;
          if (!isNull$1) {
            field$1 = in1.getInt(3);
          }
          
          ctx.element = element;
          
          
          
          
          if (isNull$1) {
            out.setField(0, null);
          } else {
            out.setField(0, field$1);
          }
                    
          
          
          if (isNull$2) {
            out.setField(1, null);
          } else {
            out.setField(1, field$2);
          }
                    
          
          
          if (isNull$3) {
            out.setField(2, null);
          } else {
            out.setField(2, field$3);
          }
                    
          
          timestamp$4 = ctx.timestamp();
          if (timestamp$4 == null) {
            throw new RuntimeException("Rowtime timestamp is not defined. Please make sure that " +
              "a proper TimestampAssigner is defined and the stream environment " +
              "uses the EventTime time characteristic.");
          }
          result$4 = org.apache.flink.table.data.TimestampData.fromEpochMillis(timestamp$4);
          isNull$4 = false;
          if (isNull$4) {
            out.setField(3, null);
          } else {
            out.setField(3, result$4);
          }
                    
                  
          output.collect(outElement.replace(out));
          ctx.element = null;
          
        }

        

        @Override
        public void close() throws Exception {
           super.close();
          
        }

        
      }
    
2023-03-07 15:59:01  [ Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0:6279 ] - [ WARN ]  
The operator name SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) exceeded the 80 characters length limit and was truncated.
2023-03-07 15:59:01  [ Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0:6285 ] - [ DEBUG ]  
Invoking Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0
2023-03-07 15:59:01  [ OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0:6293 ] - [ DEBUG ]  
Invoking OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0
2023-03-07 15:59:01  [ Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0:6303 ] - [ DEBUG ]  
-Dorg.apache.flink.shaded.netty4.io.netty.buffer.checkAccessible: true
2023-03-07 15:59:01  [ Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0:6303 ] - [ DEBUG ]  
-Dorg.apache.flink.shaded.netty4.io.netty.buffer.checkBounds: true
2023-03-07 15:59:01  [ Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0:6304 ] - [ DEBUG ]  
Loaded default ResourceLeakDetector: org.apache.flink.shaded.netty4.io.netty.util.ResourceLeakDetector@6243d550
2023-03-07 15:59:01  [ Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0:6309 ] - [ DEBUG ]  
Creating operator state backend for StreamExecCalc$9_7f86b06891c19f1e76c3f65c90ce752b_(1/1) with empty state.
2023-03-07 15:59:01  [ OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0:6309 ] - [ DEBUG ]  
Creating operator state backend for CollectSinkOperator_b58b2e9f59592b8492696c6eaf245059_(1/1) with empty state.
2023-03-07 15:59:01  [ Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0:6328 ] - [ DEBUG ]  
Creating operator state backend for SourceConversion$5_cf155f65686cb012844f7c745ec70a3c_(1/1) with empty state.
2023-03-07 15:59:01  [ Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0:6328 ] - [ DEBUG ]  
Creating operator state backend for TimestampsAndWatermarksOperator_ba40499bacce995f15693b1735928377_(1/1) with empty state.
2023-03-07 15:59:01  [ Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0:6332 ] - [ DEBUG ]  
Creating operator state backend for StreamMap_570f707193e0fe32f4d86d067aba243b_(1/1) with empty state.
2023-03-07 15:59:01  [ Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0:6332 ] - [ DEBUG ]  
Creating operator state backend for StreamSource_cbc357ccb763df2852fee8c4fc7d55f2_(1/1) with empty state.
2023-03-07 15:59:01  [ Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0:6334 ] - [ INFO ]  
Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0 (9d55e275358a411b21f20ae97427e4a6) switched from INITIALIZING to RUNNING.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-8:6335 ] - [ INFO ]  
Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1) (9d55e275358a411b21f20ae97427e4a6) switched from INITIALIZING to RUNNING.
2023-03-07 15:59:01  [ Legacy Source Thread - Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0:6336 ] - [ DEBUG ]  
Legacy source Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0 (9d55e275358a411b21f20ae97427e4a6) skip execution since the task is finished on restore
2023-03-07 15:59:01  [ Legacy Source Thread - Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0:6339 ] - [ INFO ]  
Connecting to server socket 127.0.0.1:9999
2023-03-07 15:59:01  [ OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0:6340 ] - [ INFO ]  
Initializing collect sink state with offset = 0, buffered results bytes = 0
2023-03-07 15:59:01  [ OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0:6341 ] - [ INFO ]  
Collect sink server established, address = localhost/127.0.0.1:63117
2023-03-07 15:59:01  [ OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0:6346 ] - [ DEBUG ]  
Creating operator state backend for StreamExecCalc$58_f2ba30a9177488df4b4ed4a5e3ce5903_(1/1) with empty state.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-6:6346 ] - [ INFO ]  
Received sink socket server address: localhost/127.0.0.1:63117
2023-03-07 15:59:01  [ OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0:6348 ] - [ DEBUG ]  
Creating keyed state backend for KeyedProcessOperator_337adade1e207453ed3502e01d75fd03_(1/1) with empty state.
2023-03-07 15:59:01  [ OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0:6358 ] - [ INFO ]  
Finished to build heap keyed state-backend.
2023-03-07 15:59:01  [ OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0:6365 ] - [ INFO ]  
Initializing heap keyed state backend with stream factory.
2023-03-07 15:59:01  [ OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0:6365 ] - [ DEBUG ]  
Creating operator state backend for KeyedProcessOperator_337adade1e207453ed3502e01d75fd03_(1/1) with empty state.
2023-03-07 15:59:01  [ OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0:6366 ] - [ DEBUG ]  
Compiling: BoundedOverAggregateHelper$46 

 Code:

        public final class BoundedOverAggregateHelper$46 implements org.apache.flink.table.runtime.generated.AggsHandleFunction {

          
          private final int constant$12;
          private final boolean constant$12isNull;
          
          private transient org.apache.flink.table.planner.functions.aggfunctions.MaxWithRetractAggFunction function_org$apache$flink$table$planner$functions$aggfunctions$MaxWithRetractAggFunction$d78f624eeff2a86742b5f64899608448;
          private transient org.apache.flink.table.runtime.typeutils.ExternalSerializer externalSerializer$13;
          private transient org.apache.flink.table.runtime.typeutils.ExternalSerializer externalSerializer$14;
          private org.apache.flink.table.runtime.dataview.StateMapView agg0$map_dataview;
          private org.apache.flink.table.data.binary.BinaryRawValueData agg0$map_dataview_raw_value;
          private org.apache.flink.table.runtime.dataview.StateMapView agg0$map_dataview_backup;
          private org.apache.flink.table.data.binary.BinaryRawValueData agg0$map_dataview_backup_raw_value;
          long agg1_count;
          boolean agg1_countIsNull;
          double agg2_sum;
          boolean agg2_sumIsNull;
          long agg3_count1;
          boolean agg3_count1IsNull;
          private transient org.apache.flink.table.data.conversion.StructuredObjectConverter converter$15;
          org.apache.flink.table.data.GenericRowData acc$17 = new org.apache.flink.table.data.GenericRowData(4);
          org.apache.flink.table.data.GenericRowData acc$18 = new org.apache.flink.table.data.GenericRowData(4);
          org.apache.flink.table.data.UpdatableRowData field$20;
          private org.apache.flink.table.data.RowData agg0_acc_internal;
          private org.apache.flink.table.planner.functions.aggfunctions.MaxWithRetractAggFunction.MaxWithRetractAccumulator agg0_acc_external;
          org.apache.flink.table.data.GenericRowData aggValue$45 = new org.apache.flink.table.data.GenericRowData(3);

          private org.apache.flink.table.runtime.dataview.StateDataViewStore store;

          public BoundedOverAggregateHelper$46(java.lang.Object[] references) throws Exception {
            
            
            constant$12 = ((int) 3);
            constant$12isNull = false;
            
            function_org$apache$flink$table$planner$functions$aggfunctions$MaxWithRetractAggFunction$d78f624eeff2a86742b5f64899608448 = (((org.apache.flink.table.planner.functions.aggfunctions.MaxWithRetractAggFunction) references[0]));
            externalSerializer$13 = (((org.apache.flink.table.runtime.typeutils.ExternalSerializer) references[1]));
            externalSerializer$14 = (((org.apache.flink.table.runtime.typeutils.ExternalSerializer) references[2]));
            converter$15 = (((org.apache.flink.table.data.conversion.StructuredObjectConverter) references[3]));
          }

          private org.apache.flink.api.common.functions.RuntimeContext getRuntimeContext() {
            return store.getRuntimeContext();
          }

          @Override
          public void open(org.apache.flink.table.runtime.dataview.StateDataViewStore store) throws Exception {
            this.store = store;
            
            function_org$apache$flink$table$planner$functions$aggfunctions$MaxWithRetractAggFunction$d78f624eeff2a86742b5f64899608448.open(new org.apache.flink.table.functions.FunctionContext(store.getRuntimeContext()));
                   
            
            agg0$map_dataview = (org.apache.flink.table.runtime.dataview.StateMapView) store.getStateMapView("agg0$map", false, externalSerializer$13, externalSerializer$14);
            agg0$map_dataview_raw_value = org.apache.flink.table.data.binary.BinaryRawValueData.fromObject(agg0$map_dataview);
                     
            
            agg0$map_dataview_backup = (org.apache.flink.table.runtime.dataview.StateMapView) store.getStateMapView("agg0$map", false, externalSerializer$13, externalSerializer$14);
            agg0$map_dataview_backup_raw_value = org.apache.flink.table.data.binary.BinaryRawValueData.fromObject(agg0$map_dataview_backup);
                       
            
            converter$15.open(getRuntimeContext().getUserCodeClassLoader());
                       
          }

          @Override
          public void accumulate(org.apache.flink.table.data.RowData accInput) throws Exception {
            
            double field$24;
            boolean isNull$24;
            boolean isNull$25;
            long result$26;
            boolean isNull$28;
            double result$29;
            boolean isNull$31;
            long result$32;
            isNull$24 = accInput.isNullAt(1);
            field$24 = -1.0d;
            if (!isNull$24) {
              field$24 = accInput.getDouble(1);
            }
            
            
            
            function_org$apache$flink$table$planner$functions$aggfunctions$MaxWithRetractAggFunction$d78f624eeff2a86742b5f64899608448.accumulate(agg0_acc_external, isNull$24 ? null : ((java.lang.Double) field$24));
                     
            
            long result$27 = -1L;
            boolean isNull$27;
            if (isNull$24) {
              
              isNull$27 = agg1_countIsNull;
              if (!isNull$27) {
                result$27 = agg1_count;
              }
            }
            else {
              
            
            
            isNull$25 = agg1_countIsNull || false;
            result$26 = -1L;
            if (!isNull$25) {
              
              result$26 = (long) (agg1_count + ((long) 1L));
              
            }
            
              isNull$27 = isNull$25;
              if (!isNull$27) {
                result$27 = result$26;
              }
            }
            agg1_count = result$27;;
            agg1_countIsNull = isNull$27;
                   
            
            double result$30 = -1.0d;
            boolean isNull$30;
            if (isNull$24) {
              
              isNull$30 = agg2_sumIsNull;
              if (!isNull$30) {
                result$30 = agg2_sum;
              }
            }
            else {
              
            
            
            isNull$28 = agg2_sumIsNull || isNull$24;
            result$29 = -1.0d;
            if (!isNull$28) {
              
              result$29 = (double) (agg2_sum + field$24);
              
            }
            
              isNull$30 = isNull$28;
              if (!isNull$30) {
                result$30 = result$29;
              }
            }
            agg2_sum = result$30;;
            agg2_sumIsNull = isNull$30;
                   
            
            
            
            
            isNull$31 = agg3_count1IsNull || false;
            result$32 = -1L;
            if (!isNull$31) {
              
              result$32 = (long) (agg3_count1 + ((long) 1L));
              
            }
            
            agg3_count1 = result$32;;
            agg3_count1IsNull = isNull$31;
                   
            
          }

          @Override
          public void retract(org.apache.flink.table.data.RowData retractInput) throws Exception {
            
            double field$33;
            boolean isNull$33;
            boolean isNull$34;
            long result$35;
            boolean isNull$37;
            double result$38;
            boolean isNull$40;
            long result$41;
            isNull$33 = retractInput.isNullAt(1);
            field$33 = -1.0d;
            if (!isNull$33) {
              field$33 = retractInput.getDouble(1);
            }
            
            
            
            function_org$apache$flink$table$planner$functions$aggfunctions$MaxWithRetractAggFunction$d78f624eeff2a86742b5f64899608448.retract(agg0_acc_external, isNull$33 ? null : ((java.lang.Double) field$33));
                     
            
            long result$36 = -1L;
            boolean isNull$36;
            if (isNull$33) {
              
              isNull$36 = agg1_countIsNull;
              if (!isNull$36) {
                result$36 = agg1_count;
              }
            }
            else {
              
            
            
            isNull$34 = agg1_countIsNull || false;
            result$35 = -1L;
            if (!isNull$34) {
              
              result$35 = (long) (agg1_count - ((long) 1L));
              
            }
            
              isNull$36 = isNull$34;
              if (!isNull$36) {
                result$36 = result$35;
              }
            }
            agg1_count = result$36;;
            agg1_countIsNull = isNull$36;
                   
            
            double result$39 = -1.0d;
            boolean isNull$39;
            if (isNull$33) {
              
              isNull$39 = agg2_sumIsNull;
              if (!isNull$39) {
                result$39 = agg2_sum;
              }
            }
            else {
              
            
            
            isNull$37 = agg2_sumIsNull || isNull$33;
            result$38 = -1.0d;
            if (!isNull$37) {
              
              result$38 = (double) (agg2_sum - field$33);
              
            }
            
              isNull$39 = isNull$37;
              if (!isNull$39) {
                result$39 = result$38;
              }
            }
            agg2_sum = result$39;;
            agg2_sumIsNull = isNull$39;
                   
            
            
            
            
            isNull$40 = agg3_count1IsNull || false;
            result$41 = -1L;
            if (!isNull$40) {
              
              result$41 = (long) (agg3_count1 - ((long) 1L));
              
            }
            
            agg3_count1 = result$41;;
            agg3_count1IsNull = isNull$40;
                   
                  
          }

          @Override
          public void merge(org.apache.flink.table.data.RowData otherAcc) throws Exception {
            
            throw new java.lang.RuntimeException("This function not require merge method, but the merge method is called.");
                 
          }

          @Override
          public void setAccumulators(org.apache.flink.table.data.RowData acc) throws Exception {
            
            org.apache.flink.table.data.RowData field$19;
            boolean isNull$19;
            long field$21;
            boolean isNull$21;
            double field$22;
            boolean isNull$22;
            long field$23;
            boolean isNull$23;
            isNull$19 = acc.isNullAt(0);
            field$19 = null;
            if (!isNull$19) {
              field$19 = acc.getRow(0, 3);
            }
            field$20 = null;
            if (!isNull$19) {
              field$20 = new org.apache.flink.table.data.UpdatableRowData(
                field$19,
                3);
              
            agg0$map_dataview_raw_value.setJavaObject(agg0$map_dataview);
            field$20.setField(2, agg0$map_dataview_raw_value);
                    
            }
                            
            isNull$21 = acc.isNullAt(1);
            field$21 = -1L;
            if (!isNull$21) {
              field$21 = acc.getLong(1);
            }
            isNull$23 = acc.isNullAt(3);
            field$23 = -1L;
            if (!isNull$23) {
              field$23 = acc.getLong(3);
            }
            isNull$22 = acc.isNullAt(2);
            field$22 = -1.0d;
            if (!isNull$22) {
              field$22 = acc.getDouble(2);
            }
            
            agg0_acc_internal = field$20;
            agg0_acc_external = (org.apache.flink.table.planner.functions.aggfunctions.MaxWithRetractAggFunction.MaxWithRetractAccumulator) converter$15.toExternal((org.apache.flink.table.data.RowData) agg0_acc_internal);
                  
            
            agg1_count = field$21;;
            agg1_countIsNull = isNull$21;
                     
            
            agg2_sum = field$22;;
            agg2_sumIsNull = isNull$22;
                     
            
            agg3_count1 = field$23;;
            agg3_count1IsNull = isNull$23;
                     
                
          }

          @Override
          public void resetAccumulators() throws Exception {
            
            
            
            agg0_acc_external = (org.apache.flink.table.planner.functions.aggfunctions.MaxWithRetractAggFunction.MaxWithRetractAccumulator) function_org$apache$flink$table$planner$functions$aggfunctions$MaxWithRetractAggFunction$d78f624eeff2a86742b5f64899608448.createAccumulator();
            agg0_acc_internal = (org.apache.flink.table.data.RowData) converter$15.toInternalOrNull((org.apache.flink.table.planner.functions.aggfunctions.MaxWithRetractAggFunction.MaxWithRetractAccumulator) agg0_acc_external);
                   
            
            
            agg1_count = ((long) 0L);
            agg1_countIsNull = false;
                     
            
            
            agg2_sum = ((double) 0.0d);
            agg2_sumIsNull = false;
                     
            
            
            agg3_count1 = ((long) 0L);
            agg3_count1IsNull = false;
                     
                
          }

          @Override
          public org.apache.flink.table.data.RowData getAccumulators() throws Exception {
            
            
            
            acc$18 = new org.apache.flink.table.data.GenericRowData(4);
            
            agg0_acc_internal = (org.apache.flink.table.data.RowData) converter$15.toInternalOrNull((org.apache.flink.table.planner.functions.aggfunctions.MaxWithRetractAggFunction.MaxWithRetractAccumulator) agg0_acc_external);
            if (false) {
              acc$18.setField(0, null);
            } else {
              acc$18.setField(0, agg0_acc_internal);
            }
                      
            
            
            if (agg1_countIsNull) {
              acc$18.setField(1, null);
            } else {
              acc$18.setField(1, agg1_count);
            }
                      
            
            
            if (agg2_sumIsNull) {
              acc$18.setField(2, null);
            } else {
              acc$18.setField(2, agg2_sum);
            }
                      
            
            
            if (agg3_count1IsNull) {
              acc$18.setField(3, null);
            } else {
              acc$18.setField(3, agg3_count1);
            }
                      
                    
            return acc$18;
                
          }

          @Override
          public org.apache.flink.table.data.RowData createAccumulators() throws Exception {
            
            
            
            
            acc$17 = new org.apache.flink.table.data.GenericRowData(4);
            
            org.apache.flink.table.data.RowData acc_internal$16 = (org.apache.flink.table.data.RowData) (org.apache.flink.table.data.RowData) converter$15.toInternalOrNull((org.apache.flink.table.planner.functions.aggfunctions.MaxWithRetractAggFunction.MaxWithRetractAccumulator) function_org$apache$flink$table$planner$functions$aggfunctions$MaxWithRetractAggFunction$d78f624eeff2a86742b5f64899608448.createAccumulator());
            if (false) {
              acc$17.setField(0, null);
            } else {
              acc$17.setField(0, acc_internal$16);
            }
                      
            
            
            if (false) {
              acc$17.setField(1, null);
            } else {
              acc$17.setField(1, ((long) 0L));
            }
                      
            
            
            if (false) {
              acc$17.setField(2, null);
            } else {
              acc$17.setField(2, ((double) 0.0d));
            }
                      
            
            
            if (false) {
              acc$17.setField(3, null);
            } else {
              acc$17.setField(3, ((long) 0L));
            }
                      
                    
            return acc$17;
                
          }

          @Override
          public org.apache.flink.table.data.RowData getValue() throws Exception {
            
            
            
            aggValue$45 = new org.apache.flink.table.data.GenericRowData(3);
            
            
            java.lang.Double value_external$42 = (java.lang.Double)
              function_org$apache$flink$table$planner$functions$aggfunctions$MaxWithRetractAggFunction$d78f624eeff2a86742b5f64899608448.getValue(agg0_acc_external);
            java.lang.Double value_internal$43 =
              value_external$42;
            boolean valueIsNull$44 = value_internal$43 == null;
                  
            if (valueIsNull$44) {
              aggValue$45.setField(0, null);
            } else {
              aggValue$45.setField(0, value_internal$43);
            }
                      
            
            
            if (agg1_countIsNull) {
              aggValue$45.setField(1, null);
            } else {
              aggValue$45.setField(1, agg1_count);
            }
                      
            
            
            if (agg2_sumIsNull) {
              aggValue$45.setField(2, null);
            } else {
              aggValue$45.setField(2, agg2_sum);
            }
                      
                    
            return aggValue$45;
                
          }

          @Override
          public void cleanup() throws Exception {
            
            agg0$map_dataview.clear();
                    
            
          }

          @Override
          public void close() throws Exception {
            
            function_org$apache$flink$table$planner$functions$aggfunctions$MaxWithRetractAggFunction$d78f624eeff2a86742b5f64899608448.close();
                   
          }
        }
      
2023-03-07 15:59:01  [ OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0:6424 ] - [ DEBUG ]  
Compiling: org$apache$flink$table$planner$functions$aggfunctions$MaxWithRetractAggFunction$MaxWithRetractAccumulator$0$Converter 

 Code:
public class org$apache$flink$table$planner$functions$aggfunctions$MaxWithRetractAggFunction$MaxWithRetractAccumulator$0$Converter implements org.apache.flink.table.data.conversion.DataStructureConverter {
    private final org.apache.flink.table.data.RowData.FieldGetter[] fieldGetters;
    private final org.apache.flink.table.data.conversion.DataStructureConverter[] fieldConverters;
    public org$apache$flink$table$planner$functions$aggfunctions$MaxWithRetractAggFunction$MaxWithRetractAccumulator$0$Converter(org.apache.flink.table.data.RowData.FieldGetter[] fieldGetters, org.apache.flink.table.data.conversion.DataStructureConverter[] fieldConverters) {
        this.fieldGetters = fieldGetters;
        this.fieldConverters = fieldConverters;
    }
    public java.lang.Object toInternal(java.lang.Object o) {
        final org.apache.flink.table.planner.functions.aggfunctions.MaxWithRetractAggFunction.MaxWithRetractAccumulator external = (org.apache.flink.table.planner.functions.aggfunctions.MaxWithRetractAggFunction.MaxWithRetractAccumulator) o;
        final org.apache.flink.table.data.GenericRowData genericRow = new org.apache.flink.table.data.GenericRowData(3);
        genericRow.setField(0, fieldConverters[0].toInternalOrNull(((java.lang.Double) external.max)));
        genericRow.setField(1, fieldConverters[1].toInternalOrNull(((java.lang.Long) external.mapSize)));
        genericRow.setField(2, fieldConverters[2].toInternalOrNull(((org.apache.flink.table.api.dataview.MapView) external.map)));
        return genericRow;
    }
    public java.lang.Object toExternal(java.lang.Object o) {
        final org.apache.flink.table.data.RowData internal = (org.apache.flink.table.data.RowData) o;
        final org.apache.flink.table.planner.functions.aggfunctions.MaxWithRetractAggFunction.MaxWithRetractAccumulator structured = new org.apache.flink.table.planner.functions.aggfunctions.MaxWithRetractAggFunction.MaxWithRetractAccumulator();
        structured.max = ((java.lang.Object) fieldConverters[0].toExternalOrNull(fieldGetters[0].getFieldOrNull(internal)));
        structured.mapSize = ((java.lang.Long) fieldConverters[1].toExternalOrNull(fieldGetters[1].getFieldOrNull(internal)));
        structured.map = ((org.apache.flink.table.api.dataview.MapView) fieldConverters[2].toExternalOrNull(fieldGetters[2].getFieldOrNull(internal)));
        return structured;
    }
}

2023-03-07 15:59:01  [ Thread-8:6439 ] - [ INFO ]  
Coordinator connection received
2023-03-07 15:59:01  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:6439 ] - [ INFO ]  
Sink connection established
2023-03-07 15:59:01  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:6439 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:01  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:6442 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:01  [ Thread-8:6442 ] - [ DEBUG ]  
Request received, version = , offset = 0
2023-03-07 15:59:01  [ Thread-8:6442 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:01  [ Thread-8:6442 ] - [ INFO ]  
Invalid request. Received version = , offset = 0, while expected version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:01  [ Thread-8:6442 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:01  [ channel-state-unspilling-thread-1:6444 ] - [ DEBUG ]  
OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0 (a2d74a9e1b54d9b31d4b8c09a2b51b2a)/InputChannelInfo{gateIdx=0, inputChannelIdx=0} finished recovering input.
2023-03-07 15:59:01  [ OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0:6446 ] - [ INFO ]  
OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0 (a2d74a9e1b54d9b31d4b8c09a2b51b2a) switched from INITIALIZING to RUNNING.
2023-03-07 15:59:01  [ flink-akka.actor.default-dispatcher-6:6446 ] - [ INFO ]  
OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1) (a2d74a9e1b54d9b31d4b8c09a2b51b2a) switched from INITIALIZING to RUNNING.
2023-03-07 15:59:01  [ OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0:6447 ] - [ DEBUG ]  
Converting recovered input channels (1 channels)
2023-03-07 15:59:01  [ OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0:6449 ] - [ DEBUG ]  
stopPersisting -1, lastSeenBarrier = -1 (COMPLETED) @ InputChannelInfo{gateIdx=0, inputChannelIdx=0}
2023-03-07 15:59:01  [ OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0:6449 ] - [ DEBUG ]  
LocalInputChannel [65f51bb133c7c1301453f2c303fab195#0@9d55e275358a411b21f20ae97427e4a6]: Requesting LOCAL subpartition 0 of partition 65f51bb133c7c1301453f2c303fab195#0@9d55e275358a411b21f20ae97427e4a6. ChannelStatePersister(lastSeenBarrier=-1 (COMPLETED)}
2023-03-07 15:59:01  [ OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0:6450 ] - [ DEBUG ]  
Requesting subpartition 0 of PipelinedResultPartition 65f51bb133c7c1301453f2c303fab195#0@9d55e275358a411b21f20ae97427e4a6 [PIPELINED_BOUNDED, 1 subpartitions, 2 pending consumptions].
2023-03-07 15:59:01  [ OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0:6450 ] - [ DEBUG ]  
Source: Socket Stream -> Map -> Timestamps/Watermarks -> SourceConversion(table=[default_catalog.default_database.UnnamedTable$0], fields=[product_id, buyer_name, price, date_time]) -> Calc(select=[product_id, price, date_time]) (1/1)#0 (9d55e275358a411b21f20ae97427e4a6): Creating read view for subpartition 0 of partition 65f51bb133c7c1301453f2c303fab195#0@9d55e275358a411b21f20ae97427e4a6.
2023-03-07 15:59:01  [ OverAggregate(partitionBy=[product_id], orderBy=[date_time ASC], window=[ ROWS BETWEEN 3 PRECEDING AND CURRENT ROW], select=[product_id, price, date_time, MAX(price) AS w0$o0, COUNT(price) AS w0$o1, $SUM0(price) AS w0$o2]) -> Calc(select=[product_id, w0$o0 AS max_price, (CASE((w0$o1 > 0:BIGINT), w0$o2, null:DOUBLE) / w0$o1) AS avg_price]) -> Sink: Collect table sink (1/1)#0:6450 ] - [ DEBUG ]  
Created PipelinedSubpartitionView(index: 0) of ResultPartition 65f51bb133c7c1301453f2c303fab195#0@9d55e275358a411b21f20ae97427e4a6
2023-03-07 15:59:01  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:6547 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:01  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:6548 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:01  [ Thread-8:6548 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:01  [ Thread-8:6548 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:01  [ Thread-8:6548 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:01  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:6651 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:01  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:6652 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:01  [ Thread-8:6652 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:01  [ Thread-8:6652 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:01  [ Thread-8:6652 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:01  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:6756 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:01  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:6756 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:01  [ Thread-8:6756 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:01  [ Thread-8:6756 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:01  [ Thread-8:6756 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:01  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:6858 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:01  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:6859 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:01  [ Thread-8:6859 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:01  [ Thread-8:6859 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:01  [ Thread-8:6859 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:01  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:6962 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:01  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:6962 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:01  [ Thread-8:6962 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:01  [ Thread-8:6962 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:01  [ Thread-8:6962 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:02  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:7064 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:02  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:7065 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:02  [ Thread-8:7065 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:02  [ Thread-8:7065 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:02  [ Thread-8:7065 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:02  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:7168 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:02  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:7168 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:02  [ Thread-8:7168 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:02  [ Thread-8:7168 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:02  [ Thread-8:7168 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:02  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:7270 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:02  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:7271 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:02  [ Thread-8:7271 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:02  [ Thread-8:7271 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:02  [ Thread-8:7271 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:02  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:7373 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:02  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:7374 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:02  [ Thread-8:7374 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:02  [ Thread-8:7375 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:02  [ Thread-8:7375 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:02  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:7476 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:02  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:7477 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:02  [ Thread-8:7477 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:02  [ Thread-8:7477 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:02  [ Thread-8:7477 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:02  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:7580 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:02  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:7581 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:02  [ Thread-8:7581 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:02  [ Thread-8:7581 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:02  [ Thread-8:7581 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:02  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:7684 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:02  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:7684 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:02  [ Thread-8:7684 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:02  [ Thread-8:7685 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:02  [ Thread-8:7685 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:02  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:7788 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:02  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:7789 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:02  [ Thread-8:7789 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:02  [ Thread-8:7789 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:02  [ Thread-8:7789 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:02  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:7891 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:02  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:7892 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:02  [ Thread-8:7892 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:02  [ Thread-8:7892 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:02  [ Thread-8:7892 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:02  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:7995 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:02  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:7996 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:02  [ Thread-8:7996 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:02  [ Thread-8:7996 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:02  [ Thread-8:7996 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:03  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:8099 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:03  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:8099 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:03  [ Thread-8:8099 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:03  [ Thread-8:8099 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:03  [ Thread-8:8099 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:03  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:8202 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:03  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:8203 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:03  [ Thread-8:8203 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:03  [ Thread-8:8203 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:03  [ Thread-8:8203 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:03  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:8306 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:03  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:8306 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:03  [ Thread-8:8306 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:03  [ Thread-8:8306 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:03  [ Thread-8:8306 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:03  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:8408 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:03  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:8409 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:03  [ Thread-8:8409 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:03  [ Thread-8:8409 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:03  [ Thread-8:8409 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:03  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:8511 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:03  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:8512 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:03  [ Thread-8:8512 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:03  [ Thread-8:8512 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:03  [ Thread-8:8512 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:03  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:8614 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:03  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:8615 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:03  [ Thread-8:8615 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:03  [ Thread-8:8615 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:03  [ Thread-8:8615 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:03  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:8717 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:03  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:8718 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:03  [ Thread-8:8718 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:03  [ Thread-8:8718 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:03  [ Thread-8:8718 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:03  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:8821 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:03  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:8822 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:03  [ Thread-8:8822 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:03  [ Thread-8:8822 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:03  [ Thread-8:8822 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:03  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:8924 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:03  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:8925 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:03  [ Thread-8:8925 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:03  [ Thread-8:8925 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:03  [ Thread-8:8925 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:04  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:9027 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:04  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:9028 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:04  [ Thread-8:9028 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:04  [ Thread-8:9028 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:04  [ Thread-8:9028 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:04  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:9132 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:04  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:9132 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:04  [ Thread-8:9132 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:04  [ Thread-8:9132 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:04  [ Thread-8:9132 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:04  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:9234 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:04  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:9235 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:04  [ Thread-8:9235 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:04  [ Thread-8:9235 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:04  [ Thread-8:9235 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:04  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:9337 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:04  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:9338 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:04  [ Thread-8:9338 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:04  [ Thread-8:9338 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:04  [ Thread-8:9338 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:04  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:9440 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:04  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:9441 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:04  [ Thread-8:9441 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:04  [ Thread-8:9441 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:04  [ Thread-8:9441 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:04  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:9543 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:04  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:9544 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:04  [ Thread-8:9544 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:04  [ Thread-8:9544 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:04  [ Thread-8:9544 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:04  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:9646 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:04  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:9647 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:04  [ Thread-8:9647 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:04  [ Thread-8:9647 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:04  [ Thread-8:9647 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:04  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:9749 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:04  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:9749 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:04  [ Thread-8:9749 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:04  [ Thread-8:9749 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:04  [ Thread-8:9749 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:04  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:9851 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:04  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:9852 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:04  [ Thread-8:9852 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:04  [ Thread-8:9852 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:04  [ Thread-8:9852 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:04  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:9953 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:04  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:9953 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:04  [ Thread-8:9953 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:04  [ Thread-8:9953 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:04  [ Thread-8:9953 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:05  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:10055 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:05  [ Thread-8:10057 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:05  [ Thread-8:10057 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:05  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:10057 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:05  [ Thread-8:10057 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:05  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:10158 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:05  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:10159 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:05  [ Thread-8:10159 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:05  [ Thread-8:10159 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:05  [ Thread-8:10159 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:05  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:10261 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:05  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:10262 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:05  [ Thread-8:10262 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:05  [ Thread-8:10262 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:05  [ Thread-8:10262 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:05  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:10364 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:05  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:10365 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:05  [ Thread-8:10365 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:05  [ Thread-8:10365 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:05  [ Thread-8:10365 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:05  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:10467 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:05  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:10468 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:05  [ Thread-8:10468 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:05  [ Thread-8:10468 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:05  [ Thread-8:10468 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:05  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:10570 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:05  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:10571 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:05  [ Thread-8:10571 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:05  [ Thread-8:10571 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:05  [ Thread-8:10571 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:05  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:10673 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:05  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:10674 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:05  [ Thread-8:10674 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:05  [ Thread-8:10674 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:05  [ Thread-8:10674 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:05  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:10777 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:05  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:10778 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:05  [ Thread-8:10778 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:05  [ Thread-8:10778 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:05  [ Thread-8:10778 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:05  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:10880 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:05  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:10881 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:05  [ Thread-8:10881 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:05  [ Thread-8:10881 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:05  [ Thread-8:10881 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:05  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:10983 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:05  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:10984 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:05  [ Thread-8:10984 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:05  [ Thread-8:10984 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:05  [ Thread-8:10984 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:06  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:11087 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:06  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:11087 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:06  [ Thread-8:11087 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:06  [ Thread-8:11087 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:06  [ Thread-8:11087 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:06  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:11189 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:06  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:11190 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:06  [ Thread-8:11190 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:06  [ Thread-8:11190 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:06  [ Thread-8:11190 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:06  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:11293 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:06  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:11293 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:06  [ Thread-8:11293 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:06  [ Thread-8:11293 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:06  [ Thread-8:11293 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:06  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:11395 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:06  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:11396 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:06  [ Thread-8:11396 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:06  [ Thread-8:11396 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:06  [ Thread-8:11396 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:06  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:11498 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:06  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:11499 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:06  [ Thread-8:11499 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:06  [ Thread-8:11499 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:06  [ Thread-8:11499 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:06  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:11600 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:06  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:11601 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:06  [ Thread-8:11601 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:06  [ Thread-8:11601 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:06  [ Thread-8:11601 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:06  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:11703 ] - [ DEBUG ]  
Forwarding request to sink socket server
2023-03-07 15:59:06  [ collect-sink-operator-coordinator-executor-thread-pool-thread-1:11703 ] - [ DEBUG ]  
Fetching serialized result from sink socket server
2023-03-07 15:59:06  [ Thread-8:11703 ] - [ DEBUG ]  
Request received, version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:06  [ Thread-8:11703 ] - [ DEBUG ]  
Expecting version = c6d77c9b-5d3d-46cd-afc4-966256e2ba66, offset = 0
2023-03-07 15:59:06  [ Thread-8:11704 ] - [ DEBUG ]  
Sending back 0 results
2023-03-07 15:59:06  [ TransientBlobCache shutdown hook:11770 ] - [ INFO ]  
Shutting down BLOB cache
2023-03-07 15:59:06  [ TaskExecutorStateChangelogStoragesManager shutdown hook:11770 ] - [ INFO ]  
Shutting down TaskExecutorStateChangelogStoragesManager.
2023-03-07 15:59:06  [ PermanentBlobCache shutdown hook:11770 ] - [ INFO ]  
Shutting down BLOB cache
2023-03-07 15:59:06  [ TaskExecutorLocalStateStoresManager shutdown hook:11770 ] - [ INFO ]  
Shutting down TaskExecutorLocalStateStoresManager.
2023-03-07 15:59:06  [ IOManagerAsync shutdown hook:11771 ] - [ DEBUG ]  
Shutting down I/O manager.
2023-03-07 15:59:06  [ FileChannelManagerImpl-netty-shuffle shutdown hook:11775 ] - [ INFO ]  
FileChannelManager removed spill file directory C:\Users\ADMINI~1\AppData\Local\Temp\flink-netty-shuffle-aeafdd9c-a480-4aa7-9fe2-60b96ba150df
2023-03-07 15:59:06  [ FileCache shutdown hook:11776 ] - [ INFO ]  
removed file cache directory C:\Users\ADMINI~1\AppData\Local\Temp\flink-dist-cache-6fbf0f17-c0cc-4709-a36e-c9f7ff453feb
2023-03-07 15:59:06  [ BlobServer shutdown hook:11777 ] - [ INFO ]  
Stopped BLOB server at 0.0.0.0:63048
2023-03-07 15:59:06  [ FileChannelManagerImpl-io shutdown hook:11777 ] - [ INFO ]  
FileChannelManager removed spill file directory C:\Users\ADMINI~1\AppData\Local\Temp\flink-io-442b97b6-8405-49fb-88a6-7995a0618652
